{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# II.3 QR factorisation\n\nLet $A âˆˆ â„‚^{m Ã— n}$ be a rectangular or square matrix such that $m â‰¥ n$ (i.e. more rows then columns).\nIn this chapter we consider two closely related factorisations:\n\n1. The _QR factorisation_\n$$\nA = Q R = \\underbrace{\\begin{bmatrix} ğª_1 | â‹¯ | ğª_m \\end{bmatrix}}_{Q âˆˆ U(m)} \\underbrace{\\begin{bmatrix} Ã— & â‹¯ & Ã— \\\\ & â‹± & â‹® \\\\ && Ã— \\\\ &&0 \\\\ &&â‹® \\\\ && 0 \\end{bmatrix}}_{R âˆˆ â„‚^{m Ã— n}}\n$$\nwhere $Q$ is unitary (i.e., $Q âˆˆ U(m)$, satisfying $Q^â‹†Q = I$, with columns $ğª_j âˆˆ â„‚^m$) and $R$ is _right triangular_, which means it \nis only nonzero on or to the right of the diagonal ($r_{kj} = 0$ if $k > j$).\n\n2. The _reduced QR factorisation_\n$$\nA = \\hat Q \\hat R = \\underbrace{\\begin{bmatrix} ğª_1 | â‹¯ | ğª_n \\end{bmatrix}}_{ \\hat Q âˆˆ â„‚^{m Ã— n}} \\underbrace{\\begin{bmatrix} Ã— & â‹¯ & Ã— \\\\ & â‹± & â‹® \\\\ && Ã—  \\end{bmatrix}}_{\\hat R âˆˆ â„‚^{n Ã— n}}\n$$\nwhere $Q$ has orthogonal columns ($Q^â‹† Q = I$, $ğª_j âˆˆ â„‚^m$) and $\\hat R$ is upper triangular.\n\nNote for a square matrix the reduced QR factorisation is equivalent to the QR factorisation, in which case $R$ is _upper triangular_.\nThe importance of these decomposition for square matrices is that their component pieces are easy to invert:\n$$\nA = QR \\qquad â‡’ \\qquad A^{-1}ğ› = R^{-1} Q^âŠ¤ ğ›\n$$\nand we saw in the last two chapters that triangular and orthogonal matrices are easy to invert when applied to a vector $ğ›$,\ne.g., using forward/back-substitution.\n\nFor rectangular matrices we will see that they lead to efficient solutions to the _least squares problem_: find\n$ğ±$ that minimizes the 2-norm\n$$\n\\| A ğ± - ğ› \\|.\n$$\nNote in the rectangular case the QR decomposition contains within it the reduced QR decomposition:\n$$\nA = QR = \\begin{bmatrix} \\hat Q | ğª_{n+1} | â‹¯ | ğª_m \\end{bmatrix} \\begin{bmatrix} \\hat R \\\\  ğŸ_{m-n Ã— n} \\end{bmatrix} = \\hat Q \\hat R.\n$$\n\n\n\n\nIn this lecture we discuss the followng:\n\n1. QR and least squares: We discuss the QR decomposition and its usage in solving least squares problems.\n2. Reduced QR and Gramâ€“Schmidt: We discuss computation of the Reduced QR decomposition using Gramâ€“Schmidt.\n3. Householder reflections and QR: We discuss computing the  QR decomposition using Householder reflections."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using LinearAlgebra, Plots, BenchmarkTools"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. QR and least squares\n\nHere we consider rectangular matrices with more rows than columns. Given $A âˆˆ â„‚^{m Ã— n}$ and $ğ› âˆˆ â„‚^m$,\nleast squares consists of finding a vector $ğ± âˆˆ â„‚^n$ that minimises the 2-norm:\n$\n\\| A ğ± - ğ› \\|\n$.\n\n\n**Theorem 1 (least squares via QR)** Suppose $A âˆˆ â„‚^{m Ã— n}$ has full rank. Given a QR decomposition $A = Q R$\nthen\n$$\nğ± = \\hat R^{-1} \\hat Q^â‹† ğ›\n$$\nminimises $\\| A ğ± - ğ› \\|$. \n\n**Proof**\n\nThe norm-preserving property (see PS4 Q3.1) of unitary matrices tells us\n$$\n\\| A ğ± - ğ› \\| = \\| Q R ğ± - ğ› \\| = \\| Q (R ğ± - Q^â‹† ğ›) \\| = \\| R ğ± - Q^â‹† ğ› \\| = \\left \\| \n\\begin{bmatrix} \\hat R \\\\ ğŸ_{m-n Ã— n} \\end{bmatrix} ğ± - \\begin{bmatrix} \\hat Q^â‹† \\\\ ğª_{n+1}^â‹† \\\\ â‹® \\\\ ğª_m^â‹† \\end{bmatrix}     ğ› \\right \\|\n$$\nNow note that the rows $k > n$ are independent of $ğ±$ and are a fixed contribution. Thus to minimise this norm it suffices to\ndrop them and minimise:\n$$\n\\| \\hat R ğ± - \\hat Q^â‹† ğ› \\|\n$$\nThis norm is minimised if it is zero. Provided the column rank of $A$ is full, $\\hat R$ will be invertible (Exercise: why is this?).\n\nâˆ\n\n\n**Example 1 (quadratic fit)** Suppose we want to fit noisy data by a quadratic\n$$\np(x) = pâ‚€ + pâ‚ x + pâ‚‚ x^2\n$$\nThat is, we want to choose $pâ‚€,pâ‚,pâ‚‚$ at data samples $x_1, â€¦, x_m$ so that the following is true:\n$$\npâ‚€ + pâ‚ x_k + pâ‚‚ x_k^2 â‰ˆ f_k\n$$\nwhere $f_k$ are given by data. We can reinterpret this as a least squares problem: minimise the norm\n$$\n\\left\\| \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ â‹® & â‹® & â‹® \\\\ 1 & x_m & x_m^2 \\end{bmatrix}\n\\begin{bmatrix} pâ‚€ \\\\ pâ‚ \\\\ pâ‚‚ \\end{bmatrix} - \\begin{bmatrix} f_1 \\\\ â‹® \\\\ f_m \\end{bmatrix} \\right \\|\n$$\nWe can solve this using the QR decomposition:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "m,n = 100,3\n\nx = range(0,1; length=m) # 100 points\nf = 2 .+ x .+ 2x.^2 .+ 0.1 .* randn.() # Noisy quadratic\n\nA = x .^ (0:2)'  # 100 x 3 matrix, equivalent to [ones(m) x x.^2]\nQ,\\hat R = qr(A)\n\\hat Q = Q[:,1:n] # Q represents full orthogonal matrix so we take first 3 columns\n\npâ‚€,pâ‚,pâ‚‚ = \\hat R \\ \\hat Q'f"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualise the fit:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = x -> pâ‚€ + pâ‚*x + pâ‚‚*x^2\n\nscatter(x, f; label=\"samples\", legend=:bottomright)\nplot!(x, p.(x); label=\"quadratic\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `\\` with a rectangular system does least squares by default:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A \\ f"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Reduced QR and Gramâ€“Schmidt\n\n\nHow do we compute the QR decomposition? We begin with a method\nyou may have seen before in another guise. Write\n$$\nA = \\begin{bmatrix} ğš_1 | â‹¯ | ğš_n \\end{bmatrix}\n$$\nwhere $ğš_k âˆˆ  â„‚^m$ and assume they are linearly independent ($A$ has full column rank).\n\n\n**Proposition 1 (Column spaces match)** Suppose $A = \\hat Q  \\hat R$ where $\\hat Q = [ğª_1|â€¦|ğª_n]$\nhas orthogonal columns and $\\hat R$ is upper-triangular, and $A$ has full rank.\nThen the first $j$ columns of\n$\\hat Q$ span the same space as the first $j$ columns of $A$:\n$$\n\\hbox{span}(ğš_1,â€¦,ğš_j) = \\hbox{span}(ğª_1,â€¦,ğª_j).\n$$\n\n**Proof**\n\nBecause $A$ has full rank we know $\\hat R$ is invertible, i.e. its diagonal entries do not vanish: $r_{jj} â‰ Â 0$.\nIf $ğ¯ âˆˆ \\hbox{span}(ğš_1,â€¦,ğš_j)$ we have for $ğœ âˆˆ â„‚^j$\n$$\nğ¯ = \\begin{bmatrix} ğš_1 | â‹¯ | ğš_j \\end{bmatrix} ğœ = \n\\begin{bmatrix} ğª_1 | â‹¯ | ğª_j \\end{bmatrix}  \\hat R[1:j,1:j] ğœ âˆˆ \\hbox{span}(ğª_1,â€¦,ğª_j)\n$$\n while if $ğ° âˆˆ \\hbox{span}(ğª_1,â€¦,ğª_j)$ we have for $ğ âˆˆ â„^j$\n$$\nğ° = \\begin{bmatrix} ğª_1 | â‹¯ | ğª_j \\end{bmatrix} ğ  =  \\begin{bmatrix} ğš_1 | â‹¯ | ğš_j \\end{bmatrix} \\hat R[1:j,1:j]^{-1} ğ âˆˆ  \\hbox{span}(ğš_1,â€¦,ğš_j).\n$$\n\nâˆ\n\n \nIt is possible to find $\\hat Q$ and $\\hat R$ the  using the _Gramâ€“Schmidt algorithm_.\nWe construct it column-by-column:\n\n**Algorithm 1 (Gramâ€“Schmidt)** For $j = 1, 2, â€¦, n$ define\n$$\n\\begin{align*}\nğ¯_j &:= ğš_j - âˆ‘_{k=1}^{j-1} \\underbrace{ğª_k^â‹† ğš_j}_{r_{kj}} ğª_k \\\\\nr_{jj} &:= {\\|ğ¯_j\\|} \\\\\nğª_j &:= {ğ¯_j \\over r_{jj}}\n\\end{align*}\n$$\n\n**Theorem 2 (Gramâ€“Schmidt and reduced QR)** Define $ğª_j$ and $r_{kj}$ as in Algorithm 1\n(with $r_{kj} = 0$ if $k > j$). Then a reduced QR decomposition is given by:\n$$\nA = \\underbrace{\\begin{bmatrix} ğª_1 | â‹¯ | ğª_n \\end{bmatrix}}_{ \\hat Q âˆˆ â„‚^{m Ã— n}} \\underbrace{\\begin{bmatrix} r_{11} & â‹¯ & r_{1n} \\\\ & â‹± & â‹® \\\\ && r_{nn}  \\end{bmatrix}}_{\\hat R âˆˆ â„‚^{n Ã— n}}\n$$\n\n**Proof**\n\nWe first show that $\\hat Q$ has orthogonal columns. Assume that $ğª_â„“^â‹† ğª_k = Î´_{â„“k}$ for $k,â„“ < j$. \nFor $â„“ < j$ we then have\n$$\nğª_â„“^â‹† ğ¯_j = ğª_â„“^â‹† ğš_j - âˆ‘_{k=1}^{j-1}  ğª_â„“^â‹†ğª_k ğª_k^â‹† ğš_j = 0\n$$\nhence $ğª_â„“^â‹† ğª_j = 0$ and indeed $\\hat Q$ has orthogonal columns. Further: from the definition of $ğ¯_j$ we find\n$$\nğš_j = ğ¯_j + âˆ‘_{k=1}^{j-1} r_{kj} ğª_k = âˆ‘_{k=1}^j r_{kj} ğª_k  = \\hat Q \\hat R ğ_j\n$$\n\nâˆ\n\n### Gramâ€“Schmidt in action\n\nWe are going to compute the reduced QR of a random matrix"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "m,n = 5,4\nA = randn(m,n)\nQ,\\hat R = qr(A)\n\\hat Q = Q[:,1:n]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first column of `\\hat Q` is indeed a normalised first column of `A`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "R = zeros(n,n)\nQ = zeros(m,n)\nR[1,1] = norm(A[:,1])\nQ[:,1] = A[:,1]/R[1,1]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now determine the next entries as"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "R[1,2] = Q[:,1]'A[:,2]\nv = A[:,2] - Q[:,1]*R[1,2]\nR[2,2] = norm(v)\nQ[:,2] = v/R[2,2]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the third column is then:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "R[1,3] = Q[:,1]'A[:,3]\nR[2,3] = Q[:,2]'A[:,3]\nv = A[:,3] - Q[:,1:2]*R[1:2,3]\nR[3,3] = norm(v)\nQ[:,3] = v/R[3,3]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Note the signs may not necessarily match.)\n\nWe can clean this up as a simple algorithm:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gramschmidt(A)\n    m,n = size(A)\n    m â‰¥ n || error(\"Not supported\")\n    R = zeros(n,n)\n    Q = zeros(m,n)\n    for j = 1:n\n        for k = 1:j-1\n            R[k,j] = Q[:,k]'*A[:,j]\n        end\n        v = A[:,j] - Q[:,1:j-1]*R[1:j-1,j]\n        R[j,j] = norm(v)\n        Q[:,j] = v/R[j,j]\n    end\n    Q,R\nend\n\nQ,R = gramschmidt(A)\nnorm(A - Q*R)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complexity and stability\n\nWe see within the `for j = 1:n` loop that we have $O(mj)$ operations. Thus the \ntotal complexity is $O(m n^2)$ operations.\n\n\nUnfortunately, the Gramâ€“Schmidt algorithm is _unstable_: the rounding errors when implemented in floating point\naccumulate in a way that we lose orthogonality:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = randn(300,300)\nQ,R = gramschmidt(A)\nnorm(Q'Q-I)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Householder reflections and QR\n\nAs an alternative, we will consider using Householder reflections to introduce zeros below\nthe diagonal.\nThus, if Gramâ€“Schmidt is a process of _triangular orthogonalisation_ (using triangular matrices\nto orthogonalise), Householder reflections is a process of _orthogonal triangularisation_ \n(using orthogonal matrices to triangularise).\n\nConsider multiplication by the Householder reflection corresponding to the first column,\nthat is, for\n$$\nQ_1 := Q_{ğš_1}^{\\rm H},\n$$\nconsider\n$$\nQ_1 A = \\begin{bmatrix} Ã— & Ã— & â‹¯ & Ã— \\\\\n& Ã— & â‹¯ & Ã— \\\\\n                    & â‹® & â‹± & â‹® \\\\\n                    & Ã— & â‹¯ & Ã— \\end{bmatrix} = \n\\begin{bmatrix}  Î± & ğ°^âŠ¤ \\\\ \n& A_2   \\end{bmatrix}\n$$\nwhere \n$$\nÎ± := -{\\rm csign}(a_{11})  \\|ğš_1\\|, ğ° = (Q_1 A)[1, 2:n]  \\qquad \\hbox{and} \\qquad A_2 = (Q_1 A)[2:m, 2:n],\n$$\n${\\rm csign}(z) :=  {\\rm e}^{{\\rm i} \\arg z}$. \nThat is, we have made the first column triangular.\nIn terms of an algorithm, we then introduce zeros into the first column of $A_2$,\nleaving an $A_3$, and so-on. But we can wrap this iterative algorithm into a simple\nproof by induction:\n\n**Theorem 3 (QR)** \nEvery matrix $A âˆˆ â„‚^{m Ã— n}$ has a QR factorisation:\n$$\nA = QR\n$$\nwhere $Q âˆˆ U(m)$ and $R âˆˆ â„‚^{m Ã— n}$ is right triangular.\n\n**Proof**\n\nAssume $m â‰¥ n$. If $A = [ğš_1] âˆˆ â„‚^{m Ã— 1}$ then we have for the Householder\nreflection $Q_1 = Q_{ğš_1}^{\\rm H}$\n$$\nQ_1 A = [Î± ğâ‚]\n$$\nwhich is right triangular, where $Î± = -{\\rm sign}(a_{11}) \\|ğš_1\\|$. \nIn other words \n$$\nA = \\underbrace{Q_1}_Q \\underbrace{[Î± ğâ‚]}_R.\n$$\n\nFor $n > 1$, assume every matrix with less columns than $n$ has a QR factorisation.\nFor $A = [ğš_1|â€¦|ğš_n] âˆˆ â„‚^{m Ã— n}$, let $Q_1 = Q_{ğš_1}^{\\rm H}$ so that\n$$\nQ_1 A =  \\begin{bmatrix} Î± & ğ°^âŠ¤ \\\\ & A_2 \\end{bmatrix}\n$$\nwhere $A_2 = (Q_1 A)[2:m,2:n]$ and $ğ° = (Q_1 A)[1,2:n]$. By assumption $A_2 = QÌƒ RÌƒ$. Thus we have\n$$\n\\begin{align*}\nA = Q_1 \\begin{bmatrix} Î± & ğ°^âŠ¤ \\\\ & QÌƒ RÌƒ \\end{bmatrix} \\\\\n=\\underbrace{Q_1 \\begin{bmatrix} 1 \\\\ & QÌƒ \\end{bmatrix}}_Q  \\underbrace{\\begin{bmatrix} Î± & ğ°^âŠ¤ \\\\ &  RÌƒ \\end{bmatrix}}_R.\n\\end{align*}\n$$\n\nâˆ\n\nThis proof by induction leads naturally to an iterative algorithm. Note that $QÌƒ$ is a product of all\nHouseholder reflections that come afterwards, that is, we can think of $Q$ as:\n$$\nQ = Q_1 QÌƒ_2 QÌƒ_3 â‹¯ QÌƒ_n\\qquad\\hbox{for}\\qquad QÌƒ_j = \\begin{bmatrix} I_{j-1} \\\\ & Q_j \\end{bmatrix}\n$$\nwhere $Q_j$ is a single Householder reflection corresponding to the first column of $A_j$. \nThis is stated cleanly in Julia code:\n\n**Algorithm 2 (QR via Householder)** For $A âˆˆ â„‚^{m Ã— n}$ with $m â‰¥ n$, the QR factorisation can be implemented as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function householderreflection(x)\n    y = copy(x)\n    if x[1] == 0\n        y[1] += norm(x) \n    else # note sign(z) = exp(im*angle(z)) where `angle` is the argument of a complex number\n        y[1] += sign(x[1])*norm(x) \n    end\n    w = y/norm(y)\n    I - 2*w*w'\nend\nfunction householderqr(A)\n    T = eltype(A)\n    m,n = size(A)\n    if n > m\n        error(\"More columns than rows is not supported\")\n    end\n\n    R = zeros(T, m, n)\n    Q = Matrix(one(T)*I, m, m)\n    Aâ±¼ = copy(A)\n\n    for j = 1:n\n        ğšâ‚ = Aâ±¼[:,1] # first columns of Aâ±¼\n        Qâ‚ = householderreflection(ğšâ‚)\n        Qâ‚Aâ±¼ = Qâ‚*Aâ±¼\n        Î±,ğ° = Qâ‚Aâ±¼[1,1],Qâ‚Aâ±¼[1,2:end]\n        Aâ±¼â‚Šâ‚ = Qâ‚Aâ±¼[2:end,2:end]\n\n        # populate returned data\n        R[j,j] = Î±\n        R[j,j+1:end] = ğ°\n\n        # following is equivalent to Q = Q*[I 0 ; 0 Qâ±¼]\n        Q[:,j:end] = Q[:,j:end]*Qâ‚\n\n        Aâ±¼ = Aâ±¼â‚Šâ‚ # this is the \"induction\"\n    end\n    Q,R\nend\n\nm,n = 100,50\nA = randn(m,n)\nQ,R = householderqr(A)\n@test Q'Q â‰ˆ I\n@test Q*R â‰ˆ A"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note because we are forming a full matrix representation of each Householder\nreflection this is a slow algorithm, taking $O(n^4)$ operations. The problem sheet\nwill consider a better implementation that takes $O(n^3)$ operations.\n\n\n**Example 2** We will now do an example by hand. Consider the $4 Ã— 3$ matrix\n$$\nA = \\begin{bmatrix} \n4 & 2 & -1 \\\\ \n0 & 15 & 18 \\\\\n-2 & -4 & -4 \\\\\n-2 & -4 & -10\n\\end{bmatrix}\n$$\nFor the first column we have\n$$\nQ_1 = I - {1 \\over 12} \\begin{bmatrix} 4 \\\\ 0 \\\\ -2 \\\\ -2 \\end{bmatrix} \\begin{bmatrix} 4 & 0 & -2 & -2 \\end{bmatrix} =\n{1 \\over 3} \\begin{bmatrix}\n-1 & 0 & 2 & 2 \\\\\n0 & 3 & 0 & 0 \\\\\n2 & 0 & 2 & -1 \\\\\n2 & 0 & -1 &2\n\\end{bmatrix}\n$$\nso that\n$$\nQ_1 A = \\begin{bmatrix} -3 & -6 & -9 \\\\\n & 15 & 18 \\\\\n  & 0 & 0 \\\\\n& 0 & -6\n\\end{bmatrix}\n$$\nIn this example the next column is already upper-triangular,\nbut because of our choice of reflection we will end up swapping the sign, that is\n$$\nQÌƒ_2 = \\begin{bmatrix} 1 \\\\ & -1 \\\\ && 1 \\\\ &&& 1 \\end{bmatrix}\n$$\nso that\n$$\nQÌƒ_2 Q_1 A = \\begin{bmatrix} -3 & -6 & -9 \\\\\n & -15 & -18 \\\\\n  & 0 & 0 \\\\\n& 0 & -6\n\\end{bmatrix}\n$$\nThe final reflection is\n$$\nQÌƒ_3 = \\begin{bmatrix} I_{2 Ã— 2} \\\\ &  I - \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\begin{bmatrix} 1 & -1 \\end{bmatrix} \n\\end{bmatrix} = \\begin{bmatrix} Ã¥1 \\\\ & 1 \\\\ & & 0 & 1 \\\\ & & 1 & 0 \\end{bmatrix}\n$$\ngiving us\n$$\nQÌƒ_3 QÌƒ_2 Q_1 A = \\underbrace{\\begin{bmatrix} -3 & -6 & -9 \\\\\n & -15 & -18 \\\\\n  &  & -6 \\\\\n&  & 0\n\\end{bmatrix}}_R\n$$\nThat is,\n$$\nA = Q_1 QÌƒ_2 QÌƒ_3 R = \\underbrace{{1 \\over 3} \\begin{bmatrix}\n-1 & 0 & 2 & 2 \\\\\n0 & 3 & 0 & 0 \\\\\n2 & 0 & -1 & 2 \\\\\n2 & 0 & 2 &-1\n\\end{bmatrix}}_Q \\underbrace{\\begin{bmatrix} -3 & -6 & -9 \\\\\n & -15 & -18 \\\\\n  &  & -6 \\\\\n&  & 0\n\\end{bmatrix}}_R = \\underbrace{{1 \\over 3} \\begin{bmatrix}\n-1 & 0 & 2  \\\\\n0 & 3 & 0  \\\\\n2 & 0 & -1  \\\\\n2 & 0 & 2 \n\\end{bmatrix}}_\\hat Q  \\underbrace{\\begin{bmatrix} -3 & -6 & -9 \\\\\n & -15 & -18 \\\\\n  &  & -6 \n\\end{bmatrix}}_\\hat R\n$$"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.3"
    },
    "kernelspec": {
      "name": "julia-1.8",
      "display_name": "Julia 1.8.3",
      "language": "julia"
    }
  },
  "nbformat": 4
}
