{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.7 Condition numbers\n",
    "\n",
    "We have seen that floating point arithmetic induces errors in computations, and that we can typically\n",
    "bound the absolute errors to be proportional to $C Ïµ_{\\rm m}$. We want a way to bound the\n",
    "effect of more complicated calculations like computing $A ğ±$ or $A^{-1} ğ²$ without having to deal with\n",
    "the exact nature of floating point arithmetic, as it will depend on the _data_ $A$ and $ğ±$. \n",
    "That is, we want to reduce floating point stability to a more fundamental property: _mathematical stability_:\n",
    "how does a mathematical operation like $A ğ±$ change in the precense of  small perturbations (random noise or structured\n",
    "floating point errors)?\n",
    "\n",
    "1. Backward error analysis: We introduce the concept of _backward error_ analysis, which is a more practical\n",
    "way of understanding and bounding floating point errors.\n",
    "2. Condition numbers: We introduce a _condition numbers_, which can capture the effect of perturbations in\n",
    "$A$ for linear algebra oerations. More precisely: matrix operations are mathematically _stable_ \n",
    "when the condition number is small.\n",
    "3. Bounding floating point errors for linear algebra: we see how simple operations like $A ğ±$ can be put\n",
    "into a backward error analysis framework, leading to bounds on the errors in terms of the condition number. \n",
    "\n",
    "\n",
    "## 1. Backward error analysis\n",
    "\n",
    "So far we have done forward error analysis, e.g., to understand $f(x) â‰ˆ f^{\\rm FP}(x)$ we consider either\n",
    "the absolute\n",
    "$$\n",
    "f^{\\rm FP}(x) = f(x) + Î´_{\\rm a}\n",
    "$$\n",
    "or relative\n",
    "$$\n",
    "f^{\\rm FP}(x) = f(x)(1 + Î´_{\\rm r})\n",
    "$$\n",
    "errors of the _output_. More generally, for two vector spaces $V$ and $W$ (e.g. $V = â„^n$ and $W = â„^m$) consider functions $ğŸ = V â†’ W$. We write\n",
    "$$\n",
    "ğŸ^{\\rm FP}(ğ±) = ğŸ(ğ±) + Î´_{\\rm f}\n",
    "$$\n",
    "where we bound a norm of $Î´_{\\rm f} âˆˆ W$ either _absolutely_:\n",
    "$$\n",
    "\\|Î´_{\\rm f}\\|_W â‰¤ C Îµ\n",
    "$$\n",
    "or _relative_ to the true result:\n",
    "$$\n",
    "\\|Î´_{\\rm f}\\|_W â‰¤ C \\| ğŸ(ğ±) \\|_W Îµ\n",
    "$$\n",
    "(which is similar to PS4, Q1.3).\n",
    "\n",
    "On the other hand, _backward error analysis_ \n",
    "considers computations as errors in the _input_. That is, one writes\n",
    "the approximate function as the true function with a pertubed input: e.g. find $ğ±Ìƒ âˆˆ V$ \n",
    "such that\n",
    "$$\n",
    "ğŸ^{\\rm FP}(ğ±) = ğŸ(ğ±Ìƒ).\n",
    "$$\n",
    "We study the _backward  error_, the error in the input\n",
    "$$\n",
    "ğ±Ìƒ = ğ± + Î´_{\\rm b}\n",
    "$$\n",
    "where $Î´_{\\rm b} âˆˆ â„^n$ by bounding either the absolute error\n",
    "$$\n",
    "\\|Î´_{\\rm b}\\|_V â‰¤ CÎµ\n",
    "$$\n",
    "or relative error:\n",
    "$$\n",
    "Î´_{\\rm b}\\|_V â‰¤ C \\|ğ±\\|_V Îµ\n",
    "$$\n",
    "We shall see that _some_ algorithms (like `mul_rows`) lead naturally\n",
    "to backward error results. \n",
    "\n",
    "\n",
    "\n",
    "## 2. Condition numbers\n",
    "\n",
    "So now we get to a mathematical question independent of floating point: \n",
    "can we bound the _relative error_ in approximating\n",
    "$$\n",
    "A ğ± â‰ˆ (A + Î´A) ğ±\n",
    "$$\n",
    "if we know a bound on the relative backward error   $\\|Î´A\\|$?\n",
    "It turns out we can in turns of the _condition number_ of the matrix:\n",
    "\n",
    "**Definition 2 (condition number)**\n",
    "For a square matrix $A$, the _condition number_ (in $p$-norm) is\n",
    "$$\n",
    "Îº_p(A) := \\| A \\|_p \\| A^{-1} \\|_p\n",
    "$$\n",
    "with the default being the $2$-norm condition number, writable in terms of the\n",
    "singular values as:\n",
    "$$\n",
    "Îº(A) := Îº_2(A) = \\| A \\|_2 \\| A^{-1} \\|_2 = {Ïƒ_1 \\over Ïƒ_n}.\n",
    "$$\n",
    "\n",
    "\n",
    "**Theorem 1 (relative forward error for matrix-vector)**\n",
    "Assume we have the relative backward error bound $\\|Î´A\\| â‰¤ \\|A \\| Îµ$.\n",
    "Then for \n",
    "$$\n",
    "(A + Î´A) ğ± = A ğ± + Î´_{\\rm f}\n",
    "$$\n",
    "the forward error has the relative error bound\n",
    "$$\n",
    "\\|Î´_{\\rm f}\\|  â‰¤ \\| A ğ± \\| Îº(A) Îµ\n",
    "$$\n",
    "\n",
    "\n",
    "**Proof**\n",
    "We can assume $A$ is invertible (as otherwise $Îº(A) = âˆ$). Denote $ğ² = A ğ±$ and we have\n",
    "$$\n",
    "{\\|ğ± \\| \\over \\| A ğ± \\|} = {\\|A^{-1} ğ² \\| \\over \\|ğ² \\|} â‰¤ \\| A^{-1}\\|\n",
    "$$\n",
    "Thus we have:\n",
    "$$\n",
    "{\\|Î´_{\\rm f}\\|  \\over \\| A ğ± \\|} â‰¤ {\\| Î´A \\| \\| ğ± \\| \\over \\| A ğ± \\| } â‰¤ \\underbrace{\\|A \\| \\|A^{-1}\\|}_{Îº(A)} Îµ.\n",
    "$$\n",
    "\n",
    "âˆ\n",
    "\n",
    "\n",
    "\n",
    "## 3. Bounding floating point errors for linear algebra\n",
    "\n",
    "We now observe that errors in implementing matrix-vector multiplication using floating points\n",
    "can be captured by considering the multiplication to be exact on the wrong matrix: that is, `A*x`\n",
    "(implemented with floating point as `mul_rows`) is precisely $A + Î´A$ where $Î´A$ has small norm, relative to $A$.\n",
    "That is, we have a bound on the _backward relative error_.\n",
    "\n",
    "\n",
    "\n",
    "To discuss floating point errors we need to be precise which order the operations happened.\n",
    "We will use the definition `mul_rows(A,x)` (which is equivalent to `mul_cols(A,x)`).\n",
    " Note that each entry of the result is in fact a dot-product\n",
    "of the corresponding rows so we first consider the error in the dot product  `dot(ğ±,ğ²)` as implemented in floating-point:\n",
    "$$\n",
    "{\\rm dot}(ğ±,ğ²) = â¨_{k=1}^n (x_k âŠ— y_k).\n",
    "$$\n",
    "\n",
    "We first need a helper proposition:\n",
    "\n",
    "**Proposition 1 [PS2 Q2.1]** If $|Ïµ_i| â‰¤ Ïµ$ and $n Ïµ < 1$, then\n",
    "$$\n",
    "\\prod_{k=1}^n (1+Ïµ_i) = 1+Î¸_n\n",
    "$$\n",
    "for some constant $Î¸_n$ satisfying\n",
    "$$\n",
    "|Î¸_n| â‰¤ \\underbrace{n Ïµ \\over 1-nÏµ}_{E_{n,Ïµ}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Lemma 1 (dot product backward error)**\n",
    "For $ğ±, ğ² âˆˆ â„^n$,\n",
    "$$\n",
    "{\\rm dot}(ğ±, ğ²) = (ğ± + Î´ğ±)^âŠ¤ ğ²\n",
    "$$\n",
    "where, assuming $n Ïµ_{\\rm m} < 2$, the entries satisfy\n",
    "$$\n",
    "|Î´x_k| â‰¤  E_{n,Ïµ_{\\rm m}/2} |x_k |.\n",
    "$$\n",
    "\n",
    "\n",
    "**Proof**\n",
    "\n",
    "This is related to PS2 Q2.3 but asks for the _backward error_ instead of the\n",
    "_forward error_. Note\n",
    "$$\n",
    "{\\rm dot}(ğ±, ğ²) = â¨_{j=1}^n (x_j âŠ— y_j) = â¨_{j=1}^n (x_j  y_j) (1 + Î´_j)\n",
    "= x_1 y_1 (1 + Î¸Ìƒ_{n}) +   âˆ‘_{j=2}^n x_j y_j (1 + Î¸_{n-j+2})\n",
    "$$\n",
    "where $|Î¸Ìƒ_n|, |Î¸_k| â‰¤ E_{n,Ïµ_{\\rm m}/2}$ (the subscript denotes the number of terms\n",
    "bounded by $Îµ_{\\rm m}/2$). Thus we can define\n",
    "$$\n",
    "Î´ğ±  := \\begin{bmatrix}\n",
    "x_1 Î¸Ìƒ_n \\\\\n",
    "x_2 Î¸_n \\\\\n",
    "â‹® \\\\\n",
    "x_n Î¸_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "| Î´x_k | â‰¤  E_{n,Ïµ_{\\rm m}/2} | x_k |.\n",
    "$$\n",
    "\n",
    "\n",
    "âˆ\n",
    "\n",
    "We can use this to get a relative backward error bound on `mul_rows`:\n",
    "\n",
    "**Theorem 2 (matrix-vector backward error)**\n",
    "For $A âˆˆ â„^{m Ã— n}$ and $ğ± âˆˆ â„^n$ (both with normal float entries) we have\n",
    "$$\n",
    "\\hbox{mul$\\_$rows}(A, ğ±) = (A + Î´A) ğ±\n",
    "$$\n",
    "where, assuming $n Ïµ_{\\rm m} < 2$ and all operations are in the normalised range,\n",
    "the entries (denoting $Î´a_{kj} = Î´A[k,j] = ğ_k^âŠ¤ Î´A ğ_j$) satisfy\n",
    "$$\n",
    "|Î´a_{kj}| â‰¤ E_{n,Ïµ_{\\rm m}/2}  |a_{kj}|.\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "The bound on the entries of $Î´A$ is implied by the previous lemma\n",
    "since each row is equivalent to a dot product.\n",
    "âˆ\n",
    "\n",
    "**Corollary 1 (Norms)**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\|Î´A\\|_1 &â‰¤ E_{n,Ïµ_{\\rm m}/2} \\|A \\|_1 \\\\\n",
    "\\|Î´A\\|_2 &â‰¤ \\sqrt{\\min(m,n)} E_{n,Ïµ_{\\rm m}/2} \\|A \\|_2 \\\\\n",
    "\\|Î´A\\|_âˆ &â‰¤ E_{n,Ïµ_{\\rm m}/2} \\|A \\|_âˆ\n",
    "\\end{align*}\n",
    "$$\n",
    "In particular, \n",
    "$$\n",
    "\\hbox{mul$\\_$rows}(A, ğ±) =A ğ± + Î´_{\\rm f}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\|Î´_{\\rm f}\\| â‰¤ \\|A ğ±\\| Îº(A) E_{n,Ïµ_{\\rm m}/2}\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "\n",
    "The $1$-norm follow since\n",
    "$$\n",
    "\\|Î´A\\|_1  = \\max_j âˆ‘_{k=1}^m |Î´a_{kj}| â‰¤\n",
    " E_{n,Ïµ_{\\rm m}/2}  \\max_j âˆ‘_{k=1}^m |a_{kj}|  = E_{n,Ïµ_{\\rm m}/2} \\|A\\|_1\n",
    "$$\n",
    "and the proof for the $âˆ$-norm is similar.\n",
    "\n",
    "\n",
    "This leaves the 2-norm, which is a bit more challenging.\n",
    "We will prove the result by going through the FrÃ¶benius norm and using\n",
    "$$\n",
    "\\|A \\|_2 â‰¤ \\|A\\|_F â‰¤ \\sqrt{r} \\| A\\|_2\n",
    "$$\n",
    "where $r$ is rank of $A$ (see PS6 Q5.2).\n",
    "So we deduce\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\|Î´A \\|_2^2 &â‰¤ \\| Î´A\\|_F^2 = âˆ‘_{k=1}^m âˆ‘_{j=1}^n |Î´a_{kj}|^2 â‰¤\n",
    "E_{n,Ïµ_{\\rm m}/2}^2 âˆ‘_{k=1}^m âˆ‘_{j=1}^n |a_{kj}|^2 \\\\\n",
    "&=  E_{n,Ïµ_{\\rm m}/2}^2 \\|A \\|_F^2 â‰¤ E_{n,Ïµ_{\\rm m}/2}^2 r \\|A \\|_2^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "and the rank of $A$ is bounded by $\\min(m,n)$.\n",
    "The bound on the forward error then follows from Theorem 1.\n",
    "\n",
    "âˆ\n",
    "\n",
    "\n",
    "We can also bound the error of back-substitution in terms of the condition number (see PS7).\n",
    "If one uses QR to solve $A ğ± = ğ²$ the condition number also gives a meaningful bound on the error. \n",
    "As we have already noted, there are some matrices where PLU decompositions introduce large errors, so\n",
    "in that case well-conditioning is not a guarantee  of accuracy (but it still usually works)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
