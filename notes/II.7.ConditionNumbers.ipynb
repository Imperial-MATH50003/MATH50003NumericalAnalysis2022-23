{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.7 Condition numbers\n",
    "\n",
    "We have seen that floating point arithmetic induces errors in computations, and that we can typically\n",
    "bound the absolute errors to be proportional to $C ϵ_{\\rm m}$. We want a way to bound the\n",
    "effect of more complicated calculations like computing $A 𝐱$ or $A^{-1} 𝐲$ without having to deal with\n",
    "the exact nature of floating point arithmetic, as it will depend on the _data_ $A$ and $𝐱$. \n",
    "That is, we want to reduce floating point stability to a more fundamental property: _mathematical stability_:\n",
    "how does a mathematical operation like $A 𝐱$ change in the precense of  small perturbations (random noise or structured\n",
    "floating point errors)?\n",
    "\n",
    "1. Backward error analysis: We introduce the concept of _backward error_ analysis, which is a more practical\n",
    "way of understanding and bounding floating point errors.\n",
    "2. Condition numbers: We introduce a _condition numbers_, which can capture the effect of perturbations in\n",
    "$A$ for linear algebra oerations. More precisely: matrix operations are mathematically _stable_ \n",
    "when the condition number is small.\n",
    "3. Bounding floating point errors for linear algebra: we see how simple operations like $A 𝐱$ can be put\n",
    "into a backward error analysis framework, leading to bounds on the errors in terms of the condition number. \n",
    "\n",
    "\n",
    "## 1. Backward error analysis\n",
    "\n",
    "So far we have done forward error analysis, e.g., to understand $f(x) ≈ f^{\\rm FP}(x)$ we consider either\n",
    "the absolute\n",
    "$$\n",
    "f^{\\rm FP}(x) = f(x) + δ_{\\rm a}\n",
    "$$\n",
    "or relative\n",
    "$$\n",
    "f^{\\rm FP}(x) = f(x)(1 + δ_{\\rm a})\n",
    "$$\n",
    "errors of the _output_. More generally, for vector functions $𝐟 = ℝ^n → ℝ^m$  we write\n",
    "$$\n",
    "𝐟^{\\rm FP}(𝐱) = 𝐟(𝐱) + δ_{\\rm f}\n",
    "$$\n",
    "where we bound a norm of $δ_{\\rm f} ∈ ℝ^m$ either _absolutely_:\n",
    "$$\n",
    "\\|δ_{\\rm f}\\| ≤ C ε\n",
    "$$\n",
    "or _relative_ to the true result:\n",
    "$$\n",
    "\\|δ_{\\rm f}\\| ≤ C \\| 𝐟(𝐱) \\| ε\n",
    "$$\n",
    "(which is similar to PS4, Q1.3).\n",
    "\n",
    "On the other hand, _backward error analysis_ \n",
    "considers computations as errors in the _input_. That is, one writes\n",
    "the approximate function as the true function with a pertubed input: e.g. find $𝐱̃ ∈ ℝ^n$ \n",
    "such that\n",
    "$$\n",
    "𝐟^{\\rm FP}(𝐱) = 𝐟(𝐱̃).\n",
    "$$\n",
    "We study the _backward  error_, the error in the input\n",
    "$$\n",
    "𝐱̃ = 𝐱 + δ_{\\rm b}\n",
    "$$\n",
    "where $δ_{\\rm b} ∈ ℝ^n$ by bounding either the absolute error\n",
    "$$\n",
    "\\|δ_{\\rm b}\\| ≤ Cε\n",
    "$$\n",
    "or relative error:\n",
    "$$\n",
    "δ_{\\rm b}\\| ≤ C \\|𝐱\\| ε\n",
    "$$\n",
    "We shall see that _some_ algorithms (like `mul_rows`) lead naturally\n",
    "to backward error results. \n",
    "\n",
    "\n",
    "\n",
    "## 2. Condition numbers\n",
    "\n",
    "So now we get to a mathematical question independent of floating point: \n",
    "can we bound the _relative error_ in approximating\n",
    "$$\n",
    "A 𝐱 ≈ (A + δA) 𝐱\n",
    "$$\n",
    "if we know a bound on the relative backward error   $\\|δA\\|$?\n",
    "It turns out we can in turns of the _condition number_ of the matrix:\n",
    "\n",
    "**Definition 2 (condition number)**\n",
    "For a square matrix $A$, the _condition number_ (in $p$-norm) is\n",
    "$$\n",
    "κ_p(A) := \\| A \\|_p \\| A^{-1} \\|_p\n",
    "$$\n",
    "with the default being the $2$-norm condition number, writable in terms of the\n",
    "singular values as:\n",
    "$$\n",
    "κ(A) := κ_2(A) = \\| A \\|_2 \\| A^{-1} \\|_2 = {σ_1 \\over σ_n}.\n",
    "$$\n",
    "\n",
    "\n",
    "**Theorem 1 (relative forward error for matrix-vector)**\n",
    "Assume we have the relative backward error bound $\\|δA\\| ≤ \\|A \\| ε$.\n",
    "Then for \n",
    "$$\n",
    "(A + δA) 𝐱 = A 𝐱 + δ_{\\rm f}\n",
    "$$\n",
    "the forward error has the relative error bound\n",
    "$$\n",
    "\\|δ_{\\rm f}\\|  ≤ \\| A 𝐱 \\| κ(A) ε\n",
    "$$\n",
    "\n",
    "\n",
    "**Proof**\n",
    "We can assume $A$ is invertible (as otherwise $κ(A) = ∞$). Denote $𝐲 = A 𝐱$ and we have\n",
    "$$\n",
    "{\\|𝐱 \\| \\over \\| A 𝐱 \\|} = {\\|A^{-1} 𝐲 \\| \\over \\|𝐲 \\|} ≤ \\| A^{-1}\\|\n",
    "$$\n",
    "Thus we have:\n",
    "$$\n",
    "{\\|δ_{\\rm f}\\|  \\over \\| A 𝐱 \\|} = {\\| δA \\| \\| 𝐱 \\| \\over \\| A 𝐱 \\| } ≤ \\underbrace{\\|A \\| \\|A^{-1}\\|}_{κ(A)} ε.\n",
    "$$\n",
    "\n",
    "∎\n",
    "\n",
    "\n",
    "\n",
    "## 3. Bounding floating point errors for linear algebra\n",
    "\n",
    "We now observe that errors in implementing matrix-vector multiplication using floating points\n",
    "can be captured by considering the multiplication to be exact on the wrong matrix: that is, `A*x`\n",
    "(implemented with floating point as `mul_rows`) is precisely $A + δA$ where $δA$ has small norm, relative to $A$.\n",
    "That is, we have a bound on the _backward relative error_.\n",
    "\n",
    "\n",
    "\n",
    "To discuss floating point errors we need to be precise which order the operations happened.\n",
    "We will use the definition `mul_rows(A,x)` (which is equivalent to `mul_cols(A,x)`).\n",
    " Note that each entry of the result is in fact a dot-product\n",
    "of the corresponding rows so we first consider the error in the dot product  `dot(𝐱,𝐲)` as implemented in floating-point:\n",
    "$$\n",
    "{\\rm dot}(𝐱,𝐲) = ⨁_{k=1}^n (x_k ⊗ y_k).\n",
    "$$\n",
    "\n",
    "We first need a helper proposition:\n",
    "\n",
    "**Proposition 1 [PS2 Q2.1]** If $|ϵ_i| ≤ ϵ$ and $n ϵ < 1$, then\n",
    "$$\n",
    "\\prod_{k=1}^n (1+ϵ_i) = 1+θ_n\n",
    "$$\n",
    "for some constant $θ_n$ satisfying\n",
    "$$\n",
    "|θ_n| ≤ \\underbrace{n ϵ \\over 1-nϵ}_{E_{n,ϵ}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Lemma 1 (dot product backward error)**\n",
    "For $𝐱, 𝐲 ∈ ℝ^n$,\n",
    "$$\n",
    "{\\rm dot}(𝐱, 𝐲) = (𝐱 + δ𝐱)^⊤ 𝐲\n",
    "$$\n",
    "where, assuming $n ϵ_{\\rm m} < 2$, the entries satisfy\n",
    "$$\n",
    "|δx_k| ≤  E_{n,ϵ_{\\rm m}/2} |x_k |.\n",
    "$$\n",
    "\n",
    "\n",
    "**Proof**\n",
    "\n",
    "This is related to PS2 Q2.3 but asks for the _backward error_ instead of the\n",
    "_forward error_. Note\n",
    "$$\n",
    "{\\rm dot}(𝐱, 𝐲) = ⨁_{j=1}^n (x_j ⊗ y_j) = ⨁_{j=1}^n (x_j  y_j) (1 + δ_j)\n",
    "= x_1 y_1 (1 + θ̃_{n}) +   ∑_{j=2}^n x_j y_j (1 + θ_{n-j+2})\n",
    "$$\n",
    "where $|θ̃_n|, |θ_k| ≤ E_{n,ϵ_{\\rm m}/2}$ (the subscript denotes the number of terms\n",
    "bounded by $ε_{\\rm m}/2$). Thus we can define\n",
    "$$\n",
    "δ𝐱  := \\begin{bmatrix}\n",
    "x_1 θ̃_n \\\\\n",
    "x_2 θ_n \\\\\n",
    "⋮ \\\\\n",
    "x_n θ_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "| δx_k | ≤  E_{n,ϵ_{\\rm m}/2} | x_k |.\n",
    "$$\n",
    "\n",
    "\n",
    "∎\n",
    "\n",
    "We can use this to get a relative backward error bound on `mul_rows`:\n",
    "\n",
    "**Theorem 2 (matrix-vector backward error)**\n",
    "For $A ∈ ℝ^{m × n}$ and $𝐱 ∈ ℝ^n$ we have\n",
    "$$\n",
    "\\hbox{\\tt mul\\_rows}(A, 𝐱) = (A + δA) 𝐱\n",
    "$$\n",
    "where, assuming $n ϵ_{\\rm m} < 2$, the entries (denoting $δa_{kj} = δA[k,j] = 𝐞_k^⊤ δA 𝐞_j$) \n",
    "satisfy\n",
    "$$\n",
    "|δa_{kj}| ≤ E_{n,ϵ_{\\rm m}/2}  |a_{kj}|.\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "The bound on the entries of $δA$ is implied by the previous lemma\n",
    "since each row is equivalent to a dot product.\n",
    "∎\n",
    "\n",
    "**Corollary 1 (Norms)**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\|δA\\|_1 &≤  E_{n,ϵ_{\\rm m}/2} \\|A \\|_1 \\\\\n",
    "\\|δA\\|_2 &≤  \\sqrt{\\min(m,n)} E_{n,ϵ_{\\rm m}/2} \\|A \\|_2 \\\\\n",
    "\\|δA\\|_∞ &≤  E_{n,ϵ_{\\rm m}/2} \\|A \\|_∞\n",
    "\\end{align*}\n",
    "$$\n",
    "In particular, \n",
    "$$\n",
    "\\hbox{\\tt mul\\_rows}(A, 𝐱) =A 𝐱 + δ_{\\rm f}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\|δ_{\\rm f}\\| ≤ \\|A 𝐱\\| κ(A) E_{n,ϵ_{\\rm m}/2}\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "\n",
    "The $1$-norm follow since\n",
    "$$\n",
    "\\|δA\\|_1  = \\max_j ∑_{k=1}^m |δa_{kj}| ≤\n",
    " E_{n,ϵ_{\\rm m}/2}  \\max_j ∑_{k=1}^m |a_{kj}|  = E_{n,ϵ_{\\rm m}/2} \\|A\\|_1\n",
    "$$\n",
    "and the proof for the $∞$-norm is similar.\n",
    "\n",
    "\n",
    "This leaves the 2-norm, which is a bit more challenging.\n",
    "We will prove the result by going through the Fröbenius norm and using\n",
    "$$\n",
    "\\|A \\|_2 ≤ \\|A\\|_F ≤ \\sqrt{r} \\| A\\|_2\n",
    "$$\n",
    "where $r$ is rank of $A$ (see PS6 Q5.2).\n",
    "So we deduce\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\|δA \\|_2^2 &≤ \\| δA\\|_F^2 = ∑_{k=1}^m ∑_{j=1}^n |δa_{kj}|^2 ≤\n",
    "E_{n,ϵ_{\\rm m}/2} ∑_{k=1}^m ∑_{j=1}^n |a_{kj}|^2 \\\\\n",
    "&=  E_{n,ϵ_{\\rm m}/2} \\|A \\|_F ≤ E_{n,ϵ_{\\rm m}/2} \\sqrt{r} \\|A \\|_2.\n",
    "\\end{align*}\n",
    "$$\n",
    "and the rank of $A$ is bounded by $\\min(m,n)$.\n",
    "The bound on the forward error then follows from Theorem 1. \n",
    "\n",
    "∎\n",
    "\n",
    "\n",
    "We can also bound the error of back-substitution in terms of the condition number (see PS7).\n",
    "If one uses QR to solve $A 𝐱 = 𝐲$ the condition number also gives a meaningful bound on the error. \n",
    "As we have already noted, there are some matrices where PLU decompositions introduce large errors, so\n",
    "in that case well-conditioning is not a guarantee  of accuracy (but it still usually works)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
