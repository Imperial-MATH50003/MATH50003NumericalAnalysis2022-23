**Problem 5.1** For $A = U Σ V^⊤ ∈ ℝ^{m × n}$ define the _pseudo-inverse_:
$$
A^+ := V Σ^{-1} U^⊤.
$$
Show that it satisfies the _Moore-Penrose conditions_:
1. $A A^+ A = A$
2. $A^+ A A^+ = A^+$
3. $(A A^+)^⊤  = A A^+$ and $(A^+ A)^⊤ = A^+ A$

**SOLUTION**

Let $A=UΣ V^⊤$ and $A^+ := V Σ^{-1} U^⊤$, where $U ∈ ℝ^{m × r}$ and $V ∈ ℝ^{n × r}$. Note that $U^⊤U = I_m$ and $V^⊤V = I_r$. 

1. We have
$$
A A^+ A = U Σ V^⊤ V Σ^{-1} U^⊤ U Σ V^⊤ = U Σ Σ^{-1} Σ V^⊤ = UΣ V^⊤ = A
$$
2. Moreover,
$$
A^+ A A^+ = V Σ^{-1}U^⊤ U Σ V^⊤ V Σ^{-1} U^⊤ = V Σ^{-1}Σ Σ^{-1} U^⊤ = V Σ^{-1} U^⊤ = A^+
$$
3. 
$$
\begin{align*}
(A A^+)^⊤ = (A^+)^⊤ A^⊤ = U Σ^{-1} V^⊤ V Σ U^⊤ = U U^⊤ = U Σ V^⊤ V Σ^{-1} U^⊤ = A A^+ \\
(A^+ A)^⊤ = A^⊤ (A^+)^⊤ =  V Σ U^⊤ U Σ^{-1} V^⊤ = V V^⊤ = V Σ^{-1} U^⊤ U Σ V^⊤  = A^+ A
\end{align*}
$$


**END**

**Problem 5.2** Show for $A ∈ ℝ^{m × n}$ with $m ≥ n$ and ⊤ rank
that $𝐱 =  A^+ 𝐛$ is the least squares solution, i.e., minimises $\| A 𝐱 - 𝐛 \|_2$.
Hint: extend $U$ in the SVD to be a square orthogonal matrix.

**SOLUTION**

The proof mimics that of the QR factorisation. Write $A =  U Σ V^⊤$ and let
$$
Ũ = \begin{bmatrix}U & K \end{bmatrix}
$$
so that $Ũ$ is orthogonal. We use the fact orthogonal matrices do not change norms:
$$
\begin{align*}
\|A 𝐱 - 𝐛 \|_2^2 &= \|U Σ V^⊤ 𝐱 - 𝐛 \|_2^2 = \|Ũ^⊤ U Σ V^⊤ 𝐱 - Ũ^⊤ 𝐛 \|_2^2 = \|\underbrace{\begin{bmatrix}I_m \\ O \end{bmatrix}}_{∈ ℝ^{m × n}} Σ V^⊤ 𝐱 - \begin{bmatrix} U^⊤ \\ K^⊤ \end{bmatrix} 𝐛 \|_2^2 \\
&= \|Σ V^⊤ 𝐱 - U^⊤ 𝐛 \|_2^2 + \|K^⊤ 𝐛\|^2
\end{align*}
$$
The second term is independent of $𝐱$. The first term is minimised when zero:
$$
 \|Σ V^⊤ 𝐱 - U^⊤ 𝐛 \|_2 =\|Σ V^⊤ V Σ^{-1} U^⊤ 𝐛  - U^⊤ 𝐛 \|_2 = 0
$$

**END**

**Problem 5.3**
If $A ∈ ℝ^{m × n}$ has a non-empty kernel there are multiple solutions to the least
squares problem as 
we can add any element of the kernel. Show that $𝐱 = A^+ 𝐛$ gives the least squares solution
such that $\| 𝐱 \|_2$ is minimised.

**SOLUTION**

Let $𝐱     =A^+b$ and let $𝐱 + 𝐤$ to be another solution i.e.
$$
\|A𝐱 - b \| = \|A (𝐱 +𝐤) - b \|
$$
Following the previous part we deduce:
$$
Σ V^⊤ (𝐱 +𝐤) = U^⊤ 𝐛 \Rightarrow V^⊤ 𝐤 = 0
$$
As $𝐱 = V 𝐜$ lies in the span of the columns of $V$ we have
$𝐱^⊤ 𝐤 = 0$. Thus
$$
\| 𝐱 + 𝐤 \|^2 = \| 𝐱 \|^2 + \| 𝐤 \|^2
$$
which is minimised when $𝐤 = 0$.

**END**


## 1. Condition numbers

**Problem 1.1⋆ (B)** Prove that, if $|ϵ_i| ≤ ϵ$ and $n ϵ < 1$, then
$$
\prod_{k=1}^n (1+ϵ_i) = 1+θ_n
$$
for some constant $θ_n$ satisfying $|θ_n| ≤ {n ϵ \over 1-nϵ}$.

**SOLUTION**

$$\prod_{k=1}^n(1+\epsilon_i)\le(1+\epsilon)^n=\sum_{k=0}^n {n \choose k} \epsilon^k\le 1+\sum_{k=1}^n n^k\epsilon^k\le 1+\sum_{k=1}^∞ n^k\epsilon^k=1+\frac{n\epsilon}{1-n\epsilon}.$$
$$\prod_{k=1}^n(1+\epsilon_i)\ge(1-\epsilon)^n=\sum_{k=0}^n {n \choose k} (-\epsilon)^k\ge 1-\sum_{k=1}^n n^k\epsilon^k\ge  1-\sum_{k=1}^∞ n^k\epsilon^k=1-\frac{n\epsilon}{1-n\epsilon}.$$

**Problem 1.2⋆ (B)** Let $A,B ∈ ℝ^{m × n}$. Prove that if the columns satisfy $\|𝐚_j\|_2 ≤ \| 𝐛_j\|_2$ then
$\|A\|_F ≤ \|B\|_F$ and $\|A \|_2 ≤ \sqrt{\hbox{rank}(B)} \|B\|_2$.

**SOLUTION**

Recalling from _[Problem Sheet 5](https://github.com/Imperial-MATH50003/MATH50003NumericalAnalysis/blob/main/sheets/week5s.ipynb/) - Problem 2.3* - SOLUTION_, we know that
$$\|A\|_F=\sqrt{\sum_{k,j}A[k,j]^2}=\sqrt{\sum_j\|\mathbf{a}_j\|_2^2}\qquad\text{and}\qquad\|B\|_F=\sqrt{\sum_j\|\mathbf{b}_j\|_2^2}.$$
Since $\|𝐚_j\|_2 ≤ \| 𝐛_j\|_2$, we have $\|A\|_F ≤ \|B\|_F$.

Recalling from _[Problem Sheet 5](https://github.com/Imperial-MATH50003/MATH50003NumericalAnalysis/blob/main/sheets/week5s.ipynb/) - Problem 3.1*_, we have
$$\|A\|_2\le\|A\|_F\le\|B\|_F\le\sqrt{\hbox{rank}(B)} \|B\|_2.$$

**Problem 1.3⋆ (C)** Compute the 1-norm, 2-norm, and ∞-norm condition numbers for the following matrices:
$$
\begin{bmatrix}
1 & 2 \\ 3 & 4
\end{bmatrix}, \begin{bmatrix}
1/3 & 1/5 \\ 0 & 1/7
\end{bmatrix}, \begin{bmatrix} 1 \\ & 1/2 \\ && ⋯ \\ &&& 1/2^n \end{bmatrix}
$$
(Hint: recall that the singular values of a matrix $A$ are the square roots of the eigenvalues of the Gram matrix
$A^⊤A$.)

**SOLUTION**

$$
A=\begin{bmatrix}
1 & 2 \\ 3 & 4
\end{bmatrix},\qquad
A^{-1}=-\frac{1}{2}\begin{bmatrix}
4 & -2 \\ -3 & 1
\end{bmatrix}
$$

$$
B=\begin{bmatrix}
1/3 & 1/5 \\ 0 & 1/7
\end{bmatrix},\qquad
B^{-1}=21\begin{bmatrix}
1/7 & -1/5 \\ 0 & 1/3
\end{bmatrix}
$$

$\|A\|_1=6$, $\|A^{-1}\|_1=7/2$, so $\kappa_1(A)=21$.

$\|A\|_∞=7$, $\|A^{-1}\|_∞=3$, so $\kappa_∞(A)=21$.

$\|B\|_1=12/35$, $\|B^{-1}\|_1=21\times 8/15=56/5$, so $\kappa_1(B)=96/25$.

$\|B\|_∞=8/15$, $\|B^{-1}\|_∞=21\times 12/35$, so $\kappa_∞(B)=96/25$

Finally, for the $2$-norms:
$\kappa_2(A)$:
For $A = \left[\begin{matrix}
1 & 2 \\
3 & 4
\end{matrix}\right]$, we have that the singular values are the $\sigma_1 = \sqrt{\lambda_1}, \sigma_2 = \sqrt{\lambda_2}$, where $\lambda_1$ and $\lambda_2$ are the eigenvalues of $A^TA$.
$$
A^TA = \left[\begin{matrix}
10 & 14 \\
14 & 20
\end{matrix}\right],
$$
so an eigenvalue $\lambda$ of $A^TA$ must satisfy,
$$
\begin{align*}
(10 - \lambda)(20-\lambda) - 196 = 0 \\
\Leftrightarrow \lambda = 15 \pm\sqrt{221}.
\end{align*}
$$
The larger eigenvalue corresponds to $\sigma_1$, so $\sigma_1 = \sqrt{15  + \sqrt{221}}$, and the smaller corresponds to $\sigma_2$, so $\sigma_2 = \sqrt{15  - \sqrt{221}}$. Finally, we have $\|A\|_2 = \sigma_1, \|A^{-1}\|_2 = 1/\sigma_2$, and so $\kappa_2(A) = \sqrt{\frac{15  + \sqrt{221}}{15  - \sqrt{221}}}$.

 $\kappa_2(B)$:
For 
$$
B = \left[\begin{matrix}
1/3 & 1/5 \\
0 & 1/7
\end{matrix}\right],
$$
we have that the singular values are the $\sigma_1 = \sqrt{\lambda_1}, \sigma_2 = \sqrt{\lambda_2}$, where $\lambda_1$ and $\lambda_2$ are the eigenvalues of $A^TA$.
$$
A^TA = \left[\begin{matrix}
1/9 & 1/15 \\
1/15 & \frac{74}{5^27^2}
\end{matrix}\right].
$$
An eigenvalue $\lambda$ must satisfy:
\begin{align*}
(1/9 - \lambda)\left(\frac{74}{5^27^2}-\lambda\right) - \frac{1}{225} = 0 \\
\Leftrightarrow \lambda = \frac{1891 \pm29\sqrt{2941}}{22050}.
\end{align*}
With the same logic as above, we can then deduce that $$\|B\|_2 = \sqrt{\frac{1891 +29\sqrt{2941}}{22050}}$$ and $$\|B^{-1}\|_2 \sqrt{\frac{22050}{1891 -29\sqrt{2941}}}$$ so that,
$$
\kappa_2(B) = \sqrt{\frac{1891 +29\sqrt{2941}}{1891 -29\sqrt{2941}}}
$$

For,
$$
A_n = \left[\begin{matrix}1 \\ &1/2 \\&&\ddots \\&&&1/2^n \end{matrix}\right],\hspace{5mm}
A_n^{-1} = \left[\begin{matrix}1 \\ &2 \\&&\ddots \\&&&2^n \end{matrix}\right]
$$ 
It is clear that $$\|A_n\|_1 = \|A_n\|_∞ = 1,$$ and $$\|A_n^{-1}\|_1 = \|A_n^{-1}\|_∞ = 2^n,$$ so $\kappa_1(A_n) = \kappa_∞(A) = 2^n$.
Morover, we can clearly see the singular values $\sigma_1 = 1, \sigma_2 = 1/2, \dots, \sigma_{n+1} = 1/2^n$. So $\|A_n\|_2 = 1, \|A_n^{-1}\|_2 = 2^n$, $\kappa_2(A_n) = 2^n$

**Problem 1.4 (B)**
State a bound on the relative error on $A 𝐯$ for $\|𝐯\|_2 = 1$ for the following matrices:
$$
\begin{bmatrix}
1/3 & 1/5 \\ 0 & 1/10^3
\end{bmatrix},
 \begin{bmatrix} 1 \\ & 1/2 \\ && ⋯ \\ &&& 1/2^{10} \end{bmatrix}
$$

**SOLUTION**

The Theorem (relative-error for matrix vector) tells us that,
$$
\frac{\|\delta A \mathbf{x}\|}{\|A \mathbf{x}\|} \leq \kappa(A)\epsilon,
$$
if the relative pertubation error $\|\delta A\| = \|A\| \epsilon$. For the 2-norm, we have,
$$
\|\delta A \|_2 \leq \underbrace{\frac{\sqrt{\min(m, n)} n\epsilon_m}{2 - n\epsilon_m}}_\epsilon\|A\|_2.
$$
The condition number of the first matrix is 453.33 (see code below to compute that), and $\epsilon$ defined above is $\frac{2\sqrt{2}\epsilon_m}{2-2\epsilon_m} = 3.14 \cdot10^{-16},$ so the bound on the relative error is:
$$1.42 \cdot 10^{-13}.$$
The condition number of the second matrix is $2^{10}$ by the question above, and $\epsilon$ defined above is $\frac{10\sqrt{10}\epsilon_m}{2-10\epsilon_m} = 7.02\cdot 10^{-16},$ the bound on the relative error in this case is then:
$$
7.19\cdot 10^{-13}
$$

```julia
using LinearAlgebra

A = [1/3 1/5; 0 1/1000]
U,σ,V = svd(A)
κ = σ[1]/σ[end]
v = V[:,end]
``` 
```julia
A_big = [big(1)/3 big(1)/5; 0 big(1)/1000]
``` 
```julia
norm(A_big*v - A*v, 2)/norm(A_big*v, 2)
``` 
```julia
2*sqrt(2)*eps()/(2-2*eps())* κ
``` 
```julia
B = diagm( 2.0 .^(0:-1:-10))
U,σ,V = svd(B)
κ = σ[1]/σ[end]
v = V[:,end]
``` 
```julia
B*v
``` 

Note, this is exact so the relative error is 0, within the upper bound.
```julia
10*sqrt(10)*eps()/(10-10*eps()) * 2^(10)
``` 

