# MATH50003 Numerical Analysis (2022â€“23)
# Problem Sheet 6

This problem sheet concerns Cholesky factorisations and matrix norms.


-----

**Problem 1** Use the Cholesky factorisation to determine
which of the following matrices are symmetric positive definite:
$$
\begin{bmatrix} 1 & -1  \\
-1 & 3
\end{bmatrix}, \begin{bmatrix} 1 & 2 & 2  \\
2 & 1 & 2\\
2 & 2 & 1
\end{bmatrix}, \begin{bmatrix} 3 & 2 & 1  \\
2 & 4 & 2\\
1 & 2 & 5
\end{bmatrix}, 
\begin{bmatrix} 4 & 2 & 2 & 1  \\
2 & 4 & 2 & 2\\
2 & 2 & 4 & 2 \\
1 & 2 & 2 & 4
\end{bmatrix}
$$

**SOLUTION**

A matrix is symmetric positive definite (SPD) if and only if it has a Cholesky factorisation, so the task here is really just to compute Cholesky factorisations (by hand). Since our goal is to tell if the Cholesky factorisations exist, we do not have to compute $L_k$'s. We only need to see if the factorisation process can keep to the end.

*Matrix 1*

$$A_0=\begin{bmatrix} 1 & -1  \\
-1 & 3
\end{bmatrix}$$

$A_1=3-\frac{(-1)Ã—(-1)}{1}>0$, so Matrix 1 is SPD.

*Matrix 2*

$$A_0=\begin{bmatrix}
1 & 2 & 2 \\
2 & 1 & 2 \\
2 & 2 & 1
\end{bmatrix}$$

$$A_1=\begin{bmatrix}
1&2\\
2&1
\end{bmatrix}-\begin{bmatrix} 2 \\ 2 \end{bmatrix}\begin{bmatrix} 2 & 2 \end{bmatrix}=
\begin{bmatrix}
-3&-2\\
-2&-3
\end{bmatrix}$$

$A_1[1,1] â‰¤Â 0$, so Matrix 2 is not SPD.

*Matrix 3*

$$A_0=\begin{bmatrix}
3 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 5
\end{bmatrix}$$

$$A_1=
\begin{bmatrix}
4&2\\
2&5
\end{bmatrix}-\frac{1}{3}\begin{bmatrix} 2 \\ 1 \end{bmatrix}\begin{bmatrix} 2 & 1 \end{bmatrix}=\frac{1}{3}
\begin{bmatrix}
8&4\\
4&14
\end{bmatrix}$$

$3A_2=14-\frac{4Ã— 4}{8} = 12 >0$, so Matrix 3 is SPD.

*Matrix 4*

$$A_0=\begin{bmatrix}
4 & 2 & 2 & 1 \\
2 & 4 & 2 & 2 \\
2 & 2 & 4 & 2 \\
1 & 2 & 2 & 4
\end{bmatrix}$$

$$A_1=\begin{bmatrix}
4&2&2\\
2&4&2\\
2&2&4
\end{bmatrix}-\frac{1}{4}\begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}\begin{bmatrix} 2 & 2 & 1 \end{bmatrix}=\frac{1}{4}
\begin{bmatrix}
12&4&6\\
4&12&6\\
6&6&15
\end{bmatrix}$$

$$4A_2=\begin{bmatrix}
12&6\\
6&15
\end{bmatrix}-\frac{1}{12}\begin{bmatrix} 4 \\ 6 \end{bmatrix}\begin{bmatrix} 4 & 6 \end{bmatrix}=\frac{4}{3}
\begin{bmatrix}
8&3\\
3&9
\end{bmatrix}$$
$3A_3=9-\frac{3Ã— 3}{8}>0$, so Matrix 4 is SPD.

We can check that we did this correctly by running the following in Julia:

```julia
cholesky([1 -1; -1 3])
```
```julia
# this throws an error when run because the matrix is not SPD
# @test_throws is a test that this error is thrown
@test_throws PosDefException cholesky([1 2 2; 2 1 2; 2 2 1])
```
```julia
cholesky([3 2 1; 2 4 2; 1 2 5])
```
```julia
cholesky([4 2 2 1; 2 4 2 2; 2 2 4 2; 1 2 2 4])
```
**END**

-----

**Problem 2.1** An inner product $âŸ¨ğ±, ğ²âŸ©$ on $â„^n$
satisfies, for all $ğ±,ğ²,ğ³ âˆˆ â„$ and $a,b âˆˆ â„$:
1. Symmetry: $âŸ¨ğ±, ğ²âŸ© = âŸ¨ğ², ğ±âŸ©$
2. Linearity: $âŸ¨ğ±, ağ² +bğ³âŸ© = a âŸ¨ğ±, ğ²âŸ©+ bâŸ¨ğ±, ğ³âŸ©$
3. Posive-definite: $âŸ¨ğ±, ğ±âŸ© > 0, x â‰  0$

Prove that $âŸ¨ğ±, ğ²âŸ©$ is an inner product if and only if
$$
âŸ¨ğ±, ğ²âŸ© = ğ±^âŠ¤ K ğ²
$$
where $K$ is a symmetric positive definite matrix.

**SOLUTION**

We begin by showing that $âŸ¨ğ±, ğ²âŸ© := ğ±^âŠ¤ K ğ²$ with $K$ SPD defines an inner product. To do this we simply verify the three properties:
1. Symmetry: $ âŸ¨ğ±, ğ²âŸ© = ğ±^âŠ¤ Kğ² = (ğ±^âŠ¤ Kğ²)^âŠ¤ = ğ²^âŠ¤Kğ± = âŸ¨ğ², ğ±âŸ©.$
2. Linearity: $âŸ¨ğ±, ağ² +bğ³âŸ© = ğ±^âŠ¤ K(ağ²+bğ³) = a ğ±^âŠ¤ K ğ²+bğ±^âŠ¤ K ğ³ =  a âŸ¨ğ±, ğ²âŸ©+ bâŸ¨ğ±, ğ³âŸ©$
3.  Positive-definiteness: $âŸ¨ğ±, ğ±âŸ© = ğ±^âŠ¤ K ğ± >0$. 

Now we turn to the converse result, i.e. that there exists a symmetric positive definite matrix $K$ for any inner product 
$âŸ¨ğ±, ğ²âŸ©$ such that it can be written as $âŸ¨ğ±, ğ²âŸ© = ğ±^âŠ¤ K ğ²$. Define the entries of $K$ by $K_{ij} = âŸ¨ ğ_i,  ğ_jâŸ©$ where $ ğ_j$ is the $j$-th standard basis vector. 
Then we have
$$
âŸ¨ğ±, ğ²âŸ© = âŸ¨âˆ‘_{j=1}^n x_j  ğ_j, âˆ‘_{k=1}^n y_k  ğ_k âŸ© = âˆ‘_{k=1}^n y_k âŸ¨âˆ‘_{j=1}^n x_j  ğ_j, ğ_k âŸ©
= âˆ‘_{k=1}^n âˆ‘_{j=1}^n x_j y_k âŸ¨  ğ_j, ğ_k âŸ© = âˆ‘_{k=1}^n âˆ‘_{j=1}^n x_j y_k K_{ij} = ğ±^âŠ¤ K ğ².
$$
The positive definiteness of the inner product then applies that $K$ is positive definite, e.g.:
$$
ğ±^âŠ¤ K ğ± = âŸ¨ğ±, ğ±âŸ© > 0.
$$

**END**

**Problem 2.2** Show that a symmetric positive definite matrix has strictly positive eigenvalues.
Hint: you can use the fact that symmetric matrices have real eigenvalues and eigenvectors.

**SOLUTION**
Every eigenvalue of $K$ has a unit eigenvector $ğ¯$, therefor:
$$
Î» = Î»ğ¯^âŠ¤ ğ¯ = ğ¯^âŠ¤ K ğ¯ > 0.
$$
**END**

**Problem 2.3** Show that a matrix is symmetric positive definite if and only if it has a _reverse_ Cholesky
factorisation of the form
$$
A = U U^âŠ¤
$$
where $U$ is upper triangular with positive entries on the diagonal.

**SOLUTION**

Note $ğ±^âŠ¤ U U^âŠ¤ ğ± = \| U^âŠ¤ ğ± \| > 0$ since $U$ is invertible.

For the other directrion, we replicate the proof by induction for standard Cholesky,
beginning in the bottom right
instead of the top left. Again the basis case is trivial. Thus we write:
$$
A = \begin{bmatrix} K & ğ¯\\
                    ğ¯^âŠ¤ & Î± \end{bmatrix} = 
                    \underbrace{\begin{bmatrix} I & {ğ¯ \over \sqrt{Î±}} \\
                                        & \sqrt{Î±}
                                        \end{bmatrix}}_{U_1}
                    \begin{bmatrix} K - {ğ¯ ğ¯^âŠ¤ \over Î±}  & \\
                     & 1 \end{bmatrix}
                     \underbrace{\begin{bmatrix} I \\
                      {ğ¯^âŠ¤ \over \sqrt{Î±}} & \sqrt{Î±}
                                        \end{bmatrix}}_{U_1^âŠ¤}
$$
By assumption $K - {ğ¯ ğ¯^âŠ¤ \over Î±} = UÌƒUÌƒ^âŠ¤$. 


**END**

------

**Problem 3.1â‹†** Use the Cholesky decomposition to prove that the following $n Ã— n$ matrix is symmetric positive definite
for any $n$:
$$
Î”_n := \begin{bmatrix}
2 & -1 \\
-1 & 2 & -1 \\
& -1 & 2 & â‹± \\
&& â‹± & â‹± & -1 \\
&&& -1 & 2
\end{bmatrix}
$$
Hint: replace $Î”_n[1,1]$ with $Î± > 1$ and use a proof by induction.


**SOLUTION**

Consider the first step of the Cholesky factorisation:
$$
Î”_n = \begin{bmatrix} 2 & -ğ_1^âŠ¤ \\
                    -ğ_1 & Î”_{n-1} \end{bmatrix} = 
                    \underbrace{\begin{bmatrix} \sqrt{2} \\
                                    {-ğ_1 \over \sqrt{2}} & I
                                        \end{bmatrix}}_{L_1}
                    \begin{bmatrix}1 \\ & Î”_{n-1} - {ğ_1 ğ_1^âŠ¤ \over 2} \end{bmatrix}
                    \underbrace{\begin{bmatrix} \sqrt{2} & {-ğ_1^âŠ¤ \over \sqrt{2}} \\
                                                            & I
                                        \end{bmatrix}}_{L_1^âŠ¤}
$$
The bottom right is merely $Î”_{n-1}$ but with a different $(1,1)$ entry! This hints at a strategy
of proving by induction. 

Assuming $Î± > 1$ write
$$
K_n^Î± := \begin{bmatrix}
Î± & -1 \\
-1 & 2 & -1 \\
& -1 & 2 & â‹± \\
&& â‹± & â‹± & -1 \\
&&& -1 & 2
\end{bmatrix} =
                    \begin{bmatrix} \sqrt{Î±} \\
                                    {-ğ_1 \over \sqrt{Î±}} & I
                                        \end{bmatrix}
                    \begin{bmatrix}1 \\ & K_{n-1}^{2 - 1/Î±} \end{bmatrix}
                    \begin{bmatrix} \sqrt{Î±} & {-ğ_1^âŠ¤ \over \sqrt{Î±}} \\
                                                            & I
                                        \end{bmatrix}
$$
Note if $n = 1$ this is trivially SPD. Hence assume $K_{n-1}^Î±$ is SPD for all $Î± > 1$.
If $Î± > 1$ then $2 - 1/Î± > 1$. Hence by induction and the fact that $Î”_n = K_n^2$
we conclude that $Î”_n$ has a Cholesky factorisation and hence is symmetric positive definite.

**END**

**Problem 3.2â‹†** 
Deduce its Cholesky and reverse Cholesky factorisations: $Î”_n = L_n L_n^âŠ¤ = U_n U_n^âŠ¤$ where
$L_n$ is lower triangular and $U_n$ is upper triangular.

**SOLUTION**

We can further write down the factors explicitly: define $Î±_1 := 2$ and
$$
Î±_{k+1} = 2- 1/Î±_k.
$$
Let's try out the first few:
$$
Î±_1 = 2, Î±_2 = 3/2, Î±_3 = 4/3, Î±_4 = 5/4, â€¦
$$
The pattern is clear and one can show by induction that $Î±_k = (k+1)/k$. Thus we have the Cholesky factorisation
$$
Î” _n = \underbrace{\begin{bmatrix}
\sqrt{2} \\
-1/\sqrt{2} & \sqrt{3/2} \\
& -\sqrt{2/3} & \sqrt{4/3} \\
    && â‹± & â‹± \\
    &&& -\sqrt{(n-1)/n} & \sqrt{(n+1)/n}
    \end{bmatrix}}_{L_n} \underbrace{\begin{bmatrix}
\sqrt{2} & -1/\sqrt{2} \\
 & \sqrt{3/2} & -\sqrt{2/3} \\
    && â‹± & â‹± \\
    &&& \sqrt{n/(n-1)} & -\sqrt{(n-1)/n} \\
    &&&& \sqrt{(n+1)/n}
    \end{bmatrix}}_{L_n^âŠ¤} 
$$

We can apply the same process to $U_n$, but this is a special case since flipping $\Delta_n$ horizontally and vertically gives itself: $P\Delta_nP^âŠ¤=\Delta_n$ where
$$
P=\begin{bmatrix} & & 1 \\ & â‹° & \\ 1 & & \end{bmatrix}
$$
is the permutation that reverses a vector. 
So we can also flip $L_n$ to get $U_n$:
$$
U_n=PL_nP
$$
so that $U_n U_n^âŠ¤ = P L_n P P L_n^âŠ¤ P = P Î”_n P = Î”_n$.

Alternatively one can use the procedure from Problem 2.3. That is, write:
$$
Î”_n = \begin{bmatrix} Î”_{n-1} & -ğ_n \\
                    -ğ_n^âŠ¤ & 2 \end{bmatrix} = 
                    \underbrace{\begin{bmatrix} I & {-ğ_n \over \sqrt{2}} \\
                                        & \sqrt{2}
                                        \end{bmatrix}}_{U_1}
                    \begin{bmatrix} Î”_{n-1} - {ğ_n ğ_n^âŠ¤ \over 2}  & \\
                     & 1 \end{bmatrix}
                     \underbrace{\begin{bmatrix} I \\
                      {ğ¯^âŠ¤ \over \sqrt{2}} & \sqrt{2}
                                        \end{bmatrix}}_{U_1^âŠ¤}
$$
Continuing proceeds as above.

**END**

----

**Problem 4.1** Prove the following:
$$
\begin{align*}
\|A\|_âˆ &= \max_k \|A[k,:]\|_1 \\
\|A\|_{1 â†’ âˆ} &= \|\hbox{vec}(A)\|_âˆ = \max_{kj} |a_{kj}|
\end{align*}
$$

**SOLUTION**

**Step 1. upper bounds**

$$\|Ağ±\|_âˆ=\max_k\left|âˆ‘_ja_{kj}x_j\right|\le\max_kâˆ‘_j|a_{kj}x_j|\le
\begin{cases}
\max\limits_j|x_j|\max\limits_kâˆ‘\limits_j|a_{kj}|=\|ğ±\|_âˆ\max\limits_k\|A[k,:]\|_1\\
\max\limits_{kj}|a_{kj}|âˆ‘\limits_j|x_j|=\|ğ±\|_1\|\text{vec}(A)\|_âˆ
\end{cases}
$$

**Step 2.1. meeting the upper bound ($\|A\|_{1 â†’ âˆ}$)**

Let $a_{lm}$ be the entry of $A$ with maximum absolute value. Let $ğ±=\mathbf{e}_m$, then
$$\|Ağ±\|_âˆ=\max_k\left|âˆ‘_ja_{kj}x_j\right|=\max_k|a_{km}|=|a_{lm}|$$
and
$$\|ğ±\|_1\|\text{vec}(A)\|_âˆ=1â‹…|a_{lm}|.$$


**Step 2.2. meeting the upper bound ($\|A\|_âˆ$)**

Let $A[n,:]$ be the row of $A$ with maximum 1-norm. Let $ğ±=\left(\text{sign}.(A[n,:])\right)^âŠ¤$, then $\left|âˆ‘_ja_{kj}x_j\right|\begin{cases} =âˆ‘_j|a_{kj}|=\|A[k,:]\|_1 & k=n \\ \leâˆ‘_j|a_{kj}|=\|A[k,:]\|_1 & k\ne n \end{cases}$, so
$$\|Ağ±\|_âˆ=\max_k\left|âˆ‘_ja_{kj}x_j\right|=\max\limits_k\|A[k,:]\|_1$$
while
$$\|ğ±\|_âˆ\max\limits_k\|A[k,:]\|_1=1â‹…\max\limits_k\|A[k,:]\|_1.$$


**Conclusion**

In both cases, equality can hold, so the upper bounds are actually maxima.

**END**


**Problem 4.2** For a rank-1 matrix $A = ğ± ğ²^âŠ¤$ prove that
$$
\|A \|_2 = \|ğ±\|_2 \|ğ²\|_2.
$$
Hint: use the Cauchyâ€“Schwartz inequality which states $|ğ²^âŠ¤ ğ³| â‰¤ \|ğ²\|_2\| ğ³\|_2$.

**SOLUTION**

For all $ğ³$ such that $\| ğ³ \|_2 = 1$ we have using Cauchy Schwartz
$$
\|A ğ³\|_2 = \|ğ±ğ²^âŠ¤ ğ³\|_2=|ğ²^âŠ¤ ğ³|\|ğ±\|_2 â‰¤ \|ğ²\|_2\| ğ³\|_2 \|ğ±\|_2 = \|ğ±\|_2 \|ğ²\|_2.
$$
The bound is obtained via $ğ° = ğ² / \| ğ² \|$:
$$
\|A  ğ° \| = {\|ğ± ğ²^âŠ¤ ğ² \| \over \|ğ²\|} = \| ğ± \| \| ğ² \|.
$$

----

**END**

**Problem 5.1â‹†** Show for any orthogonal matrix $Q âˆˆ â„^m$ and
matrix $A âˆˆ â„^{m Ã— n}$ that
$$
\|Q A\|_F = \|A\|_F
$$
by first showing that $\|A \|_F = \sqrt{\hbox{tr}(A^âŠ¤ A)}$ using the
_trace_ of an $m Ã— m$ matrix:
$$
\hbox{tr}(A) = a_{11} + a_{22} + â‹¯ + a_{mm}.
$$

**SOLUTION**

$$
\text{tr}(A^âŠ¤ A)=âˆ‘_k(A^âŠ¤ A)[k,k]=âˆ‘_kâˆ‘_jA^âŠ¤[k,j]A[j,k]=âˆ‘_kâˆ‘_ja_{jk}^2=\|A\|_F^2.
$$
Thus
$$
 \|A\|_F = \text{tr}(A^âŠ¤ A)=\text{tr}(A^âŠ¤ Q^âŠ¤ QA)=\text{tr}((QA)^âŠ¤ (QA))=\|QA\|_F^2.
$$

**END**



**Problem 5.2â‹†** Show that $\|A \|_2 â‰¤ \|A\|_F â‰¤Â \sqrt{r} \|A \|_2$ where
$r$ is the rank of $A$.

**SOLUTION**

From Problem 5.1 use the fact that $\|A \|_F = \sqrt{\hbox{tr}(A^âŠ¤ A)}$, where $A âˆˆ â„^{mÃ— n}$.

Hence,

$$\|A \|_F^2 = \hbox{tr}(A^âŠ¤ A) = Ïƒ_1^2 +...+Ïƒ_m^2$$

where $Ïƒ_1â‰¥...â‰¥ Ïƒ_n â‰¥ 0$ are the singular values of $A$ and $Ïƒ_i^2$ are the eigenvalues of $A^âŠ¤ A$

Knowing that $\|A\|_2^2 = Ïƒ_1^2$ we have $\|A \|_2^2 â‰¤ \|A\|_F^2$

Moreover, since if the rank of $A$ is $r$ we have that $Ïƒ_{r+1}=...=Ïƒ_m=0$ and we also know $Ïƒ_1â‰¥ ...â‰¥ Ïƒ_n â‰¥ 0$, we have that

$\|A\|_F^2 = Ïƒ_1^2 +...+Ïƒ_m^2 =Ïƒ_1^2 +...+Ïƒ_r^2 \le r Ïƒ_1^2 =r \|A \|_2^2$

Hence,
$$
\|A \|_2 â‰¤ \|A\|_F â‰¤Â \sqrt{r} \|A \|_2.
$$

**END**

