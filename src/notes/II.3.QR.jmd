# II.3 QR factorisation

Let $A âˆˆ â„‚^{m Ã— n}$ be a rectangular or square matrix such that $m â‰¥ n$ (i.e. more rows then columns).
In this chapter we consider two closely related factorisations:

1. The _QR factorisation_
$$
A = Q R = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_m \end{bmatrix}}_{Q âˆˆ U(m)} \underbrace{\begin{bmatrix} Ã— & â‹¯ & Ã— \\ & â‹± & â‹® \\ && Ã— \\ &&0 \\ &&â‹® \\ && 0 \end{bmatrix}}_{R âˆˆ â„‚^{m Ã— n}}
$$
where $Q$ is unitary (i.e., $Q âˆˆ U(m)$, satisfying $Q^â‹†Q = I$, with columns $ğª_j âˆˆ â„‚^m$) and $R$ is _right triangular_, which means it 
is only nonzero on or to the right of the diagonal ($r_{kj} = 0$ if $k > j$).

2. The _reduced QR factorisation_
$$
A = \hat Q \hat R = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_n \end{bmatrix}}_{ \hat Q âˆˆ â„‚^{m Ã— n}} \underbrace{\begin{bmatrix} Ã— & â‹¯ & Ã— \\ & â‹± & â‹® \\ && Ã—  \end{bmatrix}}_{\hat R âˆˆ â„‚^{n Ã— n}}
$$
where $Q$ has orthonormal columns ($Q^â‹† Q = I$, $ğª_j âˆˆ â„‚^m$) and $\hat R$ is upper triangular.

Note for a square matrix the reduced QR factorisation is equivalent to the QR factorisation, in which case $R$ is _upper triangular_.
The importance of these decomposition for square matrices is that their component pieces are easy to invert:
$$
A = QR \qquad â‡’ \qquad A^{-1}ğ› = R^{-1} Q^âŠ¤ ğ›
$$
and we saw in the last two chapters that triangular and orthogonal matrices are easy to invert when applied to a vector $ğ›$,
e.g., using forward/back-substitution.

For rectangular matrices we will see that they lead to efficient solutions to the _least squares problem_: find
$ğ±$ that minimizes the 2-norm
$$
\| A ğ± - ğ› \|.
$$
Note in the rectangular case the QR decomposition contains within it the reduced QR decomposition:
$$
A = QR = \begin{bmatrix} \hat Q | ğª_{n+1} | â‹¯ | ğª_m \end{bmatrix} \begin{bmatrix} \hat R \\  ğŸ_{m-n Ã— n} \end{bmatrix} = \hat Q \hat R.
$$




In this lecture we discuss the followng:

1. QR and least squares: We discuss the QR decomposition and its usage in solving least squares problems.
2. Reduced QR and Gramâ€“Schmidt: We discuss computation of the Reduced QR decomposition using Gramâ€“Schmidt.
3. Householder reflections and QR: We discuss computing the  QR decomposition using Householder reflections.

```julia
using LinearAlgebra, Plots, BenchmarkTools
```

## 1. QR and least squares

Here we consider rectangular matrices with more rows than columns. Given $A âˆˆ â„‚^{m Ã— n}$ and $ğ› âˆˆ â„‚^m$,
least squares consists of finding a vector $ğ± âˆˆ â„‚^n$ that minimises the 2-norm:
$
\| A ğ± - ğ› \|
$.


**Theorem 1 (least squares via QR)** Suppose $A âˆˆ â„‚^{m Ã— n}$ has full rank. Given a QR decomposition $A = Q R$
then
$$
ğ± = \hat R^{-1} \hat Q^â‹† ğ›
$$
minimises $\| A ğ± - ğ› \|$. 

**Proof**

The norm-preserving property (see PS4 Q3.1) of unitary matrices tells us
$$
\| A ğ± - ğ› \| = \| Q R ğ± - ğ› \| = \| Q (R ğ± - Q^â‹† ğ›) \| = \| R ğ± - Q^â‹† ğ› \| = \left \| 
\begin{bmatrix} \hat R \\ ğŸ_{m-n Ã— n} \end{bmatrix} ğ± - \begin{bmatrix} \hat Q^â‹† \\ ğª_{n+1}^â‹† \\ â‹® \\ ğª_m^â‹† \end{bmatrix}     ğ› \right \|
$$
Now note that the rows $k > n$ are independent of $ğ±$ and are a fixed contribution. Thus to minimise this norm it suffices to
drop them and minimise:
$$
\| \hat R ğ± - \hat Q^â‹† ğ› \|
$$
This norm is minimised if it is zero. Provided the column rank of $A$ is full, $\hat R$ will be invertible (Exercise: why is this?).

âˆ


**Example 1 (quadratic fit)** Suppose we want to fit noisy data by a quadratic
$$
p(x) = pâ‚€ + pâ‚ x + pâ‚‚ x^2
$$
That is, we want to choose $pâ‚€,pâ‚,pâ‚‚$ at data samples $x_1, â€¦, x_m$ so that the following is true:
$$
pâ‚€ + pâ‚ x_k + pâ‚‚ x_k^2 â‰ˆ f_k
$$
where $f_k$ are given by data. We can reinterpret this as a least squares problem: minimise the norm
$$
\left\| \begin{bmatrix} 1 & x_1 & x_1^2 \\ â‹® & â‹® & â‹® \\ 1 & x_m & x_m^2 \end{bmatrix}
\begin{bmatrix} pâ‚€ \\ pâ‚ \\ pâ‚‚ \end{bmatrix} - \begin{bmatrix} f_1 \\ â‹® \\ f_m \end{bmatrix} \right \|
$$
We can solve this using the QR decomposition:
```julia
m,n = 100,3

x = range(0,1; length=m) # 100 points
f = 2 .+ x .+ 2x.^2 .+ 0.1 .* randn.() # Noisy quadratic

A = x .^ (0:2)'  # 100 x 3 matrix, equivalent to [ones(m) x x.^2]
Q,RÌ‚ = qr(A)
QÌ‚ = Q[:,1:n] # Q represents full orthogonal matrix so we take first 3 columns

pâ‚€,pâ‚,pâ‚‚ = RÌ‚ \ QÌ‚'f
```
We can visualise the fit:
```julia
p = x -> pâ‚€ + pâ‚*x + pâ‚‚*x^2

scatter(x, f; label="samples", legend=:bottomright)
plot!(x, p.(x); label="quadratic")
```
Note that `\` with a rectangular system does least squares by default:
```julia
A \ f
```


## 2. Reduced QR and Gramâ€“Schmidt


How do we compute the QR decomposition? We begin with a method
you may have seen before in another guise. Write
$$
A = \begin{bmatrix} ğš_1 | â‹¯ | ğš_n \end{bmatrix}
$$
where $ğš_k âˆˆ  â„‚^m$ and assume they are linearly independent ($A$ has full column rank).


**Proposition 1 (Column spaces match)** Suppose $A = \hat Q  \hat R$ where $\hat Q = [ğª_1|â€¦|ğª_n]$
has orthonormal columns and $\hat R$ is upper-triangular, and $A$ has full rank.
Then the first $j$ columns of
$\hat Q$ span the same space as the first $j$ columns of $A$:
$$
\hbox{span}(ğš_1,â€¦,ğš_j) = \hbox{span}(ğª_1,â€¦,ğª_j).
$$

**Proof**

Because $A$ has full rank we know $\hat R$ is invertible, i.e. its diagonal entries do not vanish: $r_{jj} â‰ Â 0$.
If $ğ¯ âˆˆ \hbox{span}(ğš_1,â€¦,ğš_j)$ we have for $ğœ âˆˆ â„‚^j$
$$
ğ¯ = \begin{bmatrix} ğš_1 | â‹¯ | ğš_j \end{bmatrix} ğœ = 
\begin{bmatrix} ğª_1 | â‹¯ | ğª_j \end{bmatrix}  \hat R[1:j,1:j] ğœ âˆˆ \hbox{span}(ğª_1,â€¦,ğª_j)
$$
 while if $ğ° âˆˆ \hbox{span}(ğª_1,â€¦,ğª_j)$ we have for $ğ âˆˆ â„^j$
$$
ğ° = \begin{bmatrix} ğª_1 | â‹¯ | ğª_j \end{bmatrix} ğ  =  \begin{bmatrix} ğš_1 | â‹¯ | ğš_j \end{bmatrix} \hat R[1:j,1:j]^{-1} ğ âˆˆ  \hbox{span}(ğš_1,â€¦,ğš_j).
$$

âˆ

 
It is possible to find $\hat Q$ and $\hat R$ the  using the _Gramâ€“Schmidt algorithm_.
We construct it column-by-column:

**Algorithm 1 (Gramâ€“Schmidt)** For $j = 1, 2, â€¦, n$ define
$$
\begin{align*}
ğ¯_j &:= ğš_j - âˆ‘_{k=1}^{j-1} \underbrace{ğª_k^â‹† ğš_j}_{r_{kj}} ğª_k \\
r_{jj} &:= {\|ğ¯_j\|} \\
ğª_j &:= {ğ¯_j \over r_{jj}}
\end{align*}
$$

**Theorem 2 (Gramâ€“Schmidt and reduced QR)** Define $ğª_j$ and $r_{kj}$ as in Algorithm 1
(with $r_{kj} = 0$ if $k > j$). Then a reduced QR decomposition is given by:
$$
A = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_n \end{bmatrix}}_{ \hat Q âˆˆ â„‚^{m Ã— n}} \underbrace{\begin{bmatrix} r_{11} & â‹¯ & r_{1n} \\ & â‹± & â‹® \\ && r_{nn}  \end{bmatrix}}_{\hat R âˆˆ â„‚^{n Ã— n}}
$$

**Proof**

We first show that $\hat Q$ has orthonormal columns. Assume that $ğª_â„“^â‹† ğª_k = Î´_{â„“k}$ for $k,â„“ < j$. 
For $â„“ < j$ we then have
$$
ğª_â„“^â‹† ğ¯_j = ğª_â„“^â‹† ğš_j - âˆ‘_{k=1}^{j-1}  ğª_â„“^â‹†ğª_k ğª_k^â‹† ğš_j = 0
$$
hence $ğª_â„“^â‹† ğª_j = 0$ and indeed $\hat Q$ has orthonormal columns. Further: from the definition of $ğ¯_j$ we find
$$
ğš_j = ğ¯_j + âˆ‘_{k=1}^{j-1} r_{kj} ğª_k = âˆ‘_{k=1}^j r_{kj} ğª_k  = \hat Q \hat R ğ_j
$$

âˆ

### Gramâ€“Schmidt in action

We are going to compute the reduced QR of a random matrix
```julia
m,n = 5,4
A = randn(m,n)
Q,RÌ‚ = qr(A)
QÌ‚ = Q[:,1:n]
```
The first column of `\hat Q` is indeed a normalised first column of `A`:
```julia
R = zeros(n,n)
Q = zeros(m,n)
R[1,1] = norm(A[:,1])
Q[:,1] = A[:,1]/R[1,1]
```
We now determine the next entries as
```julia
R[1,2] = Q[:,1]'A[:,2]
v = A[:,2] - Q[:,1]*R[1,2]
R[2,2] = norm(v)
Q[:,2] = v/R[2,2]
```
And the third column is then:
```julia
R[1,3] = Q[:,1]'A[:,3]
R[2,3] = Q[:,2]'A[:,3]
v = A[:,3] - Q[:,1:2]*R[1:2,3]
R[3,3] = norm(v)
Q[:,3] = v/R[3,3]
```
(Note the signs may not necessarily match.)

We can clean this up as a simple algorithm:
```julia
function gramschmidt(A)
    m,n = size(A)
    m â‰¥ n || error("Not supported")
    R = zeros(n,n)
    Q = zeros(m,n)
    for j = 1:n
        for k = 1:j-1
            R[k,j] = Q[:,k]'*A[:,j]
        end
        v = A[:,j] - Q[:,1:j-1]*R[1:j-1,j]
        R[j,j] = norm(v)
        Q[:,j] = v/R[j,j]
    end
    Q,R
end

Q,R = gramschmidt(A)
norm(A - Q*R)
```


### Complexity and stability

We see within the `for j = 1:n` loop that we have $O(mj)$ operations. Thus the 
total complexity is $O(m n^2)$ operations.


Unfortunately, the Gramâ€“Schmidt algorithm is _unstable_: the rounding errors when implemented in floating point
accumulate in a way that we lose orthogonality:
```julia
A = randn(300,300)
Q,R = gramschmidt(A)
norm(Q'Q-I)
```

## 3. Householder reflections and QR

As an alternative, we will consider using Householder reflections to introduce zeros below
the diagonal.
Thus, if Gramâ€“Schmidt is a process of _triangular orthogonalisation_ (using triangular matrices
to orthogonalise), Householder reflections is a process of _orthogonal triangularisation_ 
(using orthogonal matrices to triangularise).

Consider multiplication by the Householder reflection corresponding to the first column,
that is, for
$$
Q_1 := Q_{ğš_1}^{\rm H},
$$
consider
$$
Q_1 A = \begin{bmatrix} Ã— & Ã— & â‹¯ & Ã— \\
& Ã— & â‹¯ & Ã— \\
                    & â‹® & â‹± & â‹® \\
                    & Ã— & â‹¯ & Ã— \end{bmatrix} = 
\begin{bmatrix}  Î± & ğ°^âŠ¤ \\ 
& A_2   \end{bmatrix}
$$
where 
$$
Î± := -{\rm csign}(a_{11})  \|ğš_1\|, ğ° = (Q_1 A)[1, 2:n]  \qquad \hbox{and} \qquad A_2 = (Q_1 A)[2:m, 2:n],
$$
${\rm csign}(z) :=  {\rm e}^{{\rm i} \arg z}$. 
That is, we have made the first column triangular.
In terms of an algorithm, we then introduce zeros into the first column of $A_2$,
leaving an $A_3$, and so-on. But we can wrap this iterative algorithm into a simple
proof by induction:

**Theorem 3 (QR)** 
Every matrix $A âˆˆ â„‚^{m Ã— n}$ has a QR factorisation:
$$
A = QR
$$
where $Q âˆˆ U(m)$ and $R âˆˆ â„‚^{m Ã— n}$ is right triangular.

**Proof**

Assume $m â‰¥ n$. If $A = [ğš_1] âˆˆ â„‚^{m Ã— 1}$ then we have for the Householder
reflection $Q_1 = Q_{ğš_1}^{\rm H}$
$$
Q_1 A = [Î± ğâ‚]
$$
which is right triangular, where $Î± = -{\rm sign}(a_{11}) \|ğš_1\|$. 
In other words 
$$
A = \underbrace{Q_1}_Q \underbrace{[Î± ğâ‚]}_R.
$$

For $n > 1$, assume every matrix with less columns than $n$ has a QR factorisation.
For $A = [ğš_1|â€¦|ğš_n] âˆˆ â„‚^{m Ã— n}$, let $Q_1 = Q_{ğš_1}^{\rm H}$ so that
$$
Q_1 A =  \begin{bmatrix} Î± & ğ°^âŠ¤ \\ & A_2 \end{bmatrix}
$$
where $A_2 = (Q_1 A)[2:m,2:n]$ and $ğ° = (Q_1 A)[1,2:n]$. By assumption $A_2 = \tilde Q \tilde R$. Thus we have
$$
\begin{align*}
A = Q_1 \begin{bmatrix} Î± & ğ°^âŠ¤ \\ & \tilde Q \tilde R \end{bmatrix} \\
=\underbrace{Q_1 \begin{bmatrix} 1 \\ & \tilde Q \end{bmatrix}}_Q  \underbrace{\begin{bmatrix} Î± & ğ°^âŠ¤ \\ &  \tilde R \end{bmatrix}}_R.
\end{align*}
$$

âˆ

This proof by induction leads naturally to an iterative algorithm. Note that $\tilde Q$ is a product of all
Householder reflections that come afterwards, that is, we can think of $Q$ as:
$$
Q = Q_1 \tilde Q_2 \tilde Q_3 â‹¯ \tilde Q_n\qquad\hbox{for}\qquad \tilde Q_j = \begin{bmatrix} I_{j-1} \\ & Q_j \end{bmatrix}
$$
where $Q_j$ is a single Householder reflection corresponding to the first column of $A_j$. 
This is stated cleanly in Julia code:

**Algorithm 2 (QR via Householder)** For $A âˆˆ â„‚^{m Ã— n}$ with $m â‰¥ n$, the QR factorisation can be implemented as follows:
```julia
function householderreflection(x)
    y = copy(x)
    if x[1] == 0
        y[1] += norm(x) 
    else # note sign(z) = exp(im*angle(z)) where `angle` is the argument of a complex number
        y[1] += sign(x[1])*norm(x) 
    end
    w = y/norm(y)
    I - 2*w*w'
end
function householderqr(A)
    T = eltype(A)
    m,n = size(A)
    if n > m
        error("More columns than rows is not supported")
    end

    R = zeros(T, m, n)
    Q = Matrix(one(T)*I, m, m)
    Aâ±¼ = copy(A)

    for j = 1:n
        ğšâ‚ = Aâ±¼[:,1] # first columns of Aâ±¼
        Qâ‚ = householderreflection(ğšâ‚)
        Qâ‚Aâ±¼ = Qâ‚*Aâ±¼
        Î±,ğ° = Qâ‚Aâ±¼[1,1],Qâ‚Aâ±¼[1,2:end]
        Aâ±¼â‚Šâ‚ = Qâ‚Aâ±¼[2:end,2:end]

        # populate returned data
        R[j,j] = Î±
        R[j,j+1:end] = ğ°

        # following is equivalent to Q = Q*[I 0 ; 0 Qâ±¼]
        Q[:,j:end] = Q[:,j:end]*Qâ‚

        Aâ±¼ = Aâ±¼â‚Šâ‚ # this is the "induction"
    end
    Q,R
end

m,n = 100,50
A = randn(m,n)
Q,R = householderqr(A)
@test Q'Q â‰ˆ I
@test Q*R â‰ˆ A
```


Note because we are forming a full matrix representation of each Householder
reflection this is a slow algorithm, taking $O(n^4)$ operations. The problem sheet
will consider a better implementation that takes $O(n^3)$ operations.


**Example 2 (non-examinable)** We will now do an example by hand. Consider the $4 Ã— 3$ matrix
$$
A = \begin{bmatrix} 
2 & 3 & 0 \\ 
0 & 0 & 1 \\
-2 & -3 & 0 \\
-1 & -3 & -3
\end{bmatrix}
$$
For the first column we have
$$
ğ²_1 := [-1,0,-2,-1]
$$
where $\| ğ²_1 \|^2 = 6$. Hence
$$
Q_1 := I - {1 \over 3} \begin{bmatrix} -1 \\ 0 \\ -2 \\ -1 \end{bmatrix} \begin{bmatrix} -1 & 0 & -2 & -1 \end{bmatrix} =
 {1 \over 3} \begin{bmatrix}
2 & 0 & -2 & -1 \\
0 & 3 & 0 & 0 \\
-2 & 0 & -1 & -2 \\
-1 & 0 & -2 &  2
\end{bmatrix}
$$
so that
$$
Q_1 A = \begin{bmatrix} 3 &  5 & 1 \\
 & 0 & 1 \\
  & 1 & 2 \\
& -1 & -2
\end{bmatrix}
$$
For the second column we have
$$
ğ²_2 :=  [-\sqrt{2},1,-1]
$$
where $\| ğ²_2 \|^2 = 4$. Thus we have
$$
Q_2 := I - {1 \over 2}
 \begin{bmatrix} -\sqrt{2} \\1 \\ -1
\end{bmatrix} \begin{bmatrix} -\sqrt{2} & 1 & -1 \end{bmatrix}
= \begin{bmatrix}
0 & 1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/2 & 1/2 \\
-1/\sqrt{2} & 1/2 & 1/2
\end{bmatrix}
$$
so that
$$
\tilde Q_2 Q_1 A = \begin{bmatrix} 3 & 5 & 1 \\
 & \sqrt{2} & 2\sqrt{2} \\
  & 0 & 1/\sqrt{2} \\
& 0 & -1/\sqrt{2}
\end{bmatrix}
$$
The final vector is 
$$
ğ²_3 := [1/\sqrt{2}-1,-1/\sqrt{2}]
$$
where $\| ğ²_3 \|^2 = 2 - 2/\sqrt{2}$. Hence
$$
Q_3 := I - {\sqrt{2} \over \sqrt{2} - 1} \begin{bmatrix}
1/\sqrt{2}-1 \\
-1/\sqrt{2}
\end{bmatrix} \begin{bmatrix}
1/\sqrt{2}-1 &
-1/\sqrt{2}
\end{bmatrix} =
\begin{bmatrix}
\sqrt{2} & -\sqrt{2}\\
-\sqrt{2} & -\sqrt{2}
\end{bmatrix}
$$
so that 
$$
\tilde Q_3 \tilde Q_2 Q_1 A = \begin{bmatrix} 3 & 5 & 1 \\
 & \sqrt{2} & 2\sqrt{2} \\
  & 0 & 1 \\
& 0 & 0
\end{bmatrix} =: R
$$
and
$$
Q := Q_1 \tilde Q_2 \tilde Q_3 =  \begin{bmatrix}
2/3 & -1/(3\sqrt{2}) & 0 & 1/\sqrt{2} \\
0 &  0 & 1 & 0 \\
-2/3 & 1/(3\sqrt{2}) & 0 & 1/\sqrt{2} \\ 
-1/3 & - 4/(3\sqrt{2}) & 0 & 0
\end{bmatrix}.
$$
