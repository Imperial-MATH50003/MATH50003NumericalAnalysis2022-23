# II.6 Singular value decomposition and conditioning


In this chapter we discuss the _Singular Value Decomposition_: a matrix factorisation
that encodes how much a matrix "stretches" a random vector. This includes _singular values_, 
the largest of which dictates the $2$-norm of the matirx.

In particular, we discuss:

1. Singular value decomposition: we introduce the singular value decomposition which is related to
the matrix $2$-norm and best low rank approximation.
2. Condition numbers: we will see how errors in matrix-vector multiplication and solving linear systems
can be bounded in terms of the _condition number_, which is defined in terms of singular values.

```julia
using LinearAlgebra, Plots
```




## 1. Singular value decomposition

To define the induced $2$-norm we need to consider the following:

**Definition 1 (singular value decomposition)** For $A âˆˆ â„‚^{m Ã— n}$ with rank $r > 0$, 
the _(reduced) singular value decomposition (SVD)_ is
$$
A = U Î£ V^â‹†
$$
where $U âˆˆ â„‚^{m Ã— r}$ and $V âˆˆ  â„‚^{r Ã— n}$ have orthonormal columns and $Î£ âˆˆ â„^{r Ã— r}$ is  diagonal whose
diagonal entries, which which we call _singular values_, are all positive and non-increasing: $Ïƒ_1 â‰¥ â‹¯ â‰¥ Ïƒ_r > 0$.
The _full singular value decomposition (SVD)_ is
$$
A = UÌƒ Î£Ìƒ VÌƒ^âŠ¤
$$
where $UÌƒ âˆˆ U(m)$ and $VÌƒ âˆˆ  U(n)$ are unitary matrices and $Î£Ìƒ âˆˆ â„^{m Ã— n}$ has only
diagonal non-zero entries, i.e., if $m > n$,
$$
Î£Ìƒ = \begin{bmatrix} Ïƒ_1 \\ & â‹± \\ && Ïƒ_n \\ && 0 \\ && â‹® \\ && 0 \end{bmatrix}
$$
and if $m < n$,
$$
Î£Ìƒ = \begin{bmatrix} Ïƒ_1 \\ & â‹± \\ && Ïƒ_m & 0 & â‹¯ & 0\end{bmatrix}
$$
where $Ïƒ_k = 0$ if $k > r$.

To show the SVD exists we first establish some properties of a _Gram matrix_ ($A^â‹† A$):

**Proposition 1 (Gram matrix kernel)** The kernel of $A$ is the also the kernel of $A^â‹† A$. 

**Proof**
If $A^â‹† A ğ± = 0$ then we have
$$
0 = ğ±^â‹† A^â‹† A ğ± = \| A ğ± \|^2
$$
which means $A ğ± = 0$ and $ğ± âˆˆ \hbox{ker}(A)$.
âˆ

**Proposition 2 (Gram matrix diagonalisation)** The Gram-matrix
satisfies
$$
A^â‹† A = Q Î› Q^â‹† âˆˆ â„‚^{n Ã— n}
$$
is a Hermitian matrix where $Q âˆˆ U(n)$ and the eigenvalues $Î»_k$ are real and non-negative.
If $A âˆˆ â„^{m Ã— n}$ then $Q âˆˆ O(n)$.

**Proof**
$A^â‹† A$ is Hermitian so we appeal to the spectral theorem for the
existence of the decomposition, and the fact that the eigenvalues are real.
For the corresponding (orthonormal) eigenvector $ğª_k$,
$$
Î»_k = Î»_k ğª_k^â‹† ğª_k = ğª_k^â‹† A^â‹† A ğª_k = \| A ğª_k \|^2 â‰¥ 0.
$$

âˆ


This connection allows us to prove existence:

**Theorem 1 (SVD existence)** Every $A âˆˆ â„‚^{m Ã— n}$ has an SVD.

**Proof**
Consider
$$
A^â‹† A = Q Î› Q^â‹†.
$$
Assume (as usual) that the eigenvalues are sorted in decreasing modulus, and so $Î»_1,â€¦,Î»_r$
are an enumeration of the non-zero eigenvalues and
$$
V := \begin{bmatrix} ğª_1 | â‹¯ | ğª_r \end{bmatrix}
$$
the corresponding (orthonormal) eigenvectors, with
$$
K = \begin{bmatrix} ğª_{r+1} | â‹¯ | ğª_n \end{bmatrix}
$$
the corresponding kernel. 
Define
$$
Î£ :=  \begin{bmatrix} \sqrt{Î»_1} \\ & â‹± \\ && \sqrt{Î»_r} \end{bmatrix}
$$
Now define
$$
U := AV Î£^{-1}
$$
which is orthogonal since $A^â‹† A V = V Î£^2 $:
$$
U^â‹† U = Î£^{-1} V^â‹† A^â‹† A V Î£^{-1} = I.
$$
Thus we have
$$
U Î£ V^â‹† = A V V^â‹† = A \underbrace{\begin{bmatrix} V | K \end{bmatrix}}_Q\underbrace{\begin{bmatrix} V^â‹† \\ K^â‹† \end{bmatrix}}_{Q^â‹†}
$$
where we use the fact that $A K = 0$ so that concatenating $K$ does not change the value.

âˆ

Singular values tell us the 2-norm:

**Corollary (singular values and norm)**
$$
\|A \|_2 = Ïƒ_1
$$
and if $A âˆˆ â„‚^{n Ã— n}$ is invertible, then
$$
\|A^{-1} \|_2 = Ïƒ_n^{-1}
$$

**Proof**

First we establish the upper-bound:
$$
\|A \|_2 â‰¤Â  \|U \|_2 \| Î£ \|_2 \| V^âŠ¤\|_2 = \| Î£ \|_2  = Ïƒ_1
$$
This is attained using the first right singular vector:
$$
\|A ğ¯_1\|_2 = \|Î£ V^âŠ¤ ğ¯_1\|_2 = \|Î£  ğ_1\|_2 = Ïƒ_1
$$
The inverse result follows since the inverse has SVD
$$
A^{-1} = V Î£^{-1} U^âŠ¤ = V (W Î£^{-1} W) U^âŠ¤
$$
is the SVD of $A^{-1}$, where
$$
W := P_Ïƒ = \begin{bmatrix} && 1 \\ & â‹° \\ 1 \end{bmatrix}
$$
is the permutation that reverses the entries, that is, $Ïƒ$ has Cauchy notation
$$
\begin{pmatrix}
1 & 2 & â‹¯ & n \\
n & n-1 & â‹¯ & 1
\end{pmatrix}.
$$


âˆ

We will not discuss in this module computation of singular value decompositions or eigenvalues:
they involve iterative algorithms (actually built on a sequence of QR decompositions).

One of the main usages for SVDs is low-rank approximation:

**Theorem 2 (best low rank approximation)** The  matrix
$$
A_k := \begin{bmatrix} ğ®_1 | â‹¯ | ğ®_k \end{bmatrix} \begin{bmatrix}
Ïƒ_1 \\
& â‹± \\
&& Ïƒ_k\end{bmatrix} \begin{bmatrix} ğ¯_1 | â‹¯ | ğ¯_k \end{bmatrix}^âŠ¤
$$ 
is the best 2-norm approximation of $A$ by a rank $k$ matrix, that is, for all rank-$k$ matrices $B$, we have 
$$\|A - A_k\|_2 â‰¤ \|A -B \|_2.$$


**Proof**
We have

$$
A - A_k = U \begin{bmatrix} 0  \cr &\ddots \cr && 0 \cr &&& Ïƒ_{k+1} \cr &&&& \ddots \cr &&&&& Ïƒ_r\end{bmatrix} V^âŠ¤.
$$
Suppose a rank-$k$ matrix $B$ has 
$$
\|A-B\|_2  < \|A-A_k\|_2 = Ïƒ_{k+1}.
$$
For all $ğ° \in \ker(B)$ we have 
$$
\|A ğ°\|_2 = \|(A-B) ğ°\|_2 â‰¤ \|A-B\|\|ğ°\|_2  < Ïƒ_{k+1} \|ğ°\|_2
$$

But for all $ğ® \in {\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$, that is, $ğ® = V[:,1:k+1]ğœ$ for some $ğœ \in â„^{k+1}$  we have 
$$
\|A ğ®\|_2^2 = \|U Î£_k ğœ\|_2^2 = \|Î£_k ğœ\|_2^2 =
\sum_{j=1}^{k+1} (Ïƒ_j c_j)^2 â‰¥ Ïƒ_{k+1}^2 \|c\|^2,
$$
i.e., $\|A ğ®\|_2 â‰¥ Ïƒ_{k+1} \|c\|$.  Thus $ğ°$ cannot be in this span.


The dimension of the span of $\ker(B)$ is at least $n-k$, but the dimension of ${\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$ is at least $k+1$.
Since these two spaces cannot intersect we have a contradiction, since $(n-r) + (r+1) = n+1 > n$.  âˆ



**Example 1 (Hilbert matrix)** Here we show an example of a simple low-rank approximation using the SVD. Consider the Hilbert matrix:
```julia
hilbertmatrix(n) = [1/(k+j-1) for j = 1:n, k=1:n]
hilbertmatrix(5)
```
That is, the $H[k,j] = 1/(k+j-1)$. This is a famous example of matrix with rapidly decreasing singular values:
```julia
H = hilbertmatrix(100)
U,Ïƒ,V = svd(H)
scatter(Ïƒ; yscale=:log10)
```
Note numerically we typically do not get a exactly zero singular values so the rank is always
treated as $\min(m,n)$.
Because the singular values decay rapidly 
 we can approximate the matrix very well with a rank 20 matrix:
```julia
k = 20 # rank
Î£_k = Diagonal(Ïƒ[1:k])
U_k = U[:,1:k]
V_k = V[:,1:k]
norm(U_k * Î£_k * V_k' - H)
```

Note that this can be viewed as a _compression_ algorithm: we have replaced a matrix with 
$100^2 = 10,000$ entries by two matrices and a vector with $4,000$ entries without losing
any information.
In the problem sheet we explore the usage of low rank approximation to smooth functions
and to compress images.


## 2. Condition numbers

We have seen that floating point arithmetic induces errors in computations, and that we can typically
bound the absolute errors to be proportional to $C Ïµ_{\rm m}$. We want a way to bound the
effect of more complicated calculations like computing $A ğ±$ or $A^{-1} ğ²$ without having to deal with
the exact nature of floating point arithmetic. Here we consider only matrix-multiplication but will make a remark
about matrix inversion.

To justify what follows, we first observe that errors in implementing matrix-vector multiplication
can be captured by considering the multiplication to be exact on the wrong matrix: that is, `A*x`
(implemented with floating point) is precisely $A + Î´A$ where $Î´A$ has small norm, relative to $A$.
This is known as _backward error analysis_.



To discuss floating point errors we need to be precise which order the operations happened.
We will use the definition `mul(A,x)`, which denote ${\rm mul}(A, ğ±)$. (Note that `mul_rows` actually
does the _exact_ same operations, just in a different order.) Note that each entry of the result is in fact a dot-product
of the corresponding rows so we first consider the error in the dot product  `dot(ğ±,ğ²)` as implemented in floating-point, 
which we denote ${\rm dot}(A,x)$.

We first need a helper proposition, from PS2 Q2.1:

**Proposition 3** If $|Ïµ_i| â‰¤ Ïµ$ and $n Ïµ < 1$, then
$$
\prod_{k=1}^n (1+Ïµ_i) = 1+Î¸_n
$$
for some constant $Î¸_n$ satisfying $|Î¸_n| â‰¤ E_{n,Ïµ} := {n Ïµ \over 1-nÏµ}$.



**Lemma 1 (dot product backward error)**
For $ğ±, ğ² âˆˆ â„^n$,
$$
{\rm dot}(ğ±, ğ²) = (ğ± + Î´ğ±)^âŠ¤ ğ²
$$
where
$$
|Î´ğ±| â‰¤Â  E_{n,Ïµ_{\rm m}/2} |ğ± |,
$$
where $|ğ± |$ means absolute-value of each entry, assuming $n Ïµ_{\rm m} < 2$.


**Proof**

This is related to PS2 Q2.3 but asks for the _backward error_ instead of the
_forward error_. Note
$$
{\rm dot}(ğ±, ğ²) = â¨_{j=1}^n (x_j âŠ— y_j) = â¨_{j=1}^n (x_j  y_j) (1 + Î´_j)
= x_1 y_1 (1 + Î¸Ìƒ_{n}) +   âˆ‘_{j=2}^n x_j y_j (1 + Î¸_{n-j+2})
$$
where $|Î¸Ìƒ_n|, |Î¸_k| â‰¤Â E_{n,Ïµ_{\rm m}/2}$ (the subscript denotes the number of terms
bounded by $Îµ_{\rm m}/2$. Thus we can define
$$
Î´ğ±  := \begin{bmatrix}
x_1 Î¸Ìƒ_n \\
x_2 Î¸_n \\
â‹® \\
x_n Î¸_2
\end{bmatrix}
$$
where
$$
| Î´ğ± | â‰¤Â E_{n,Ïµ_{\rm m}/2} | ğ± |.
$$


âˆ

**Theorem 3 (matrix-vector backward error)**
For $A âˆˆ â„^{m Ã— n}$ and $ğ± âˆˆ â„^n$ we have
$$
{\rm mul_rows}(A, ğ±) = (A + Î´A) ğ±
$$
where
$$
|Î´A| â‰¤ E_{n,Ïµ_{\rm m}/2}  |A|,
$$
assuming $n Ïµ_{\rm m} < 2$. Therefore
$$
\begin{align*}
\|Î´A\|_1 &â‰¤Â  E_{n,Ïµ_{\rm m}/2} \|A \|_1 \\
\|Î´A\|_2 &â‰¤Â  \sqrt{\min(m,n)} E_{n,Ïµ_{\rm m}/2} \|A \|_2 \\
\|Î´A\|_âˆ &â‰¤Â  E_{n,Ïµ_{\rm m}/2} \|A \|_âˆ
\end{align*}
$$

**Proof**
The bound on $|Î´A|$ is implied by the previous lemma.
The $1$ and $âˆ$-norm follow since
$$
\|A\|_1 = \||A|\|_1 \hbox{ and } \|A\|_âˆ = \||A|\|_âˆ
$$
This leaves the 2-norm example, which is a bit more challenging as there are matrices
$A$ such that $\|A\|_2 â‰ Â \| |A| \|_2$.
Instead we will prove the result by going through the FrÃ¶benius norm and using:
$$
\|A \|_2 â‰¤ \|A\|_F â‰¤Â \sqrt{r} \| A\|_2
$$
where $r$ is rank of $A$ (see PS6 Q5.2)
and $\|A\|_F = \| |A| \|_F$,
so we deduce:
$$
\begin{align*}
\|Î´A \|_2 &â‰¤Â \| Î´A\|F = \| |Î´A| \|F â‰¤Â E_{n,Ïµ_{\rm m}/2} \| |A| \|_F \\
          &= E_{n,Ïµ_{\rm m}/2} \| A \|_F â‰¤Â \sqrt{r} E_{n,Ïµ_{\rm m}/2} \| A \|_2 \\
          &â‰¤ \sqrt{\min(m,n)} E_{n,Ïµ_{\rm m}/2} \|A \|_2
\end{align*}
$$

âˆ

So now we get to a mathematical question independent of floating point: 
can we bound the _relative error_ in approximating
$$
A ğ± â‰ˆ (A + Î´A) ğ±
$$
if we know a bound on $\|Î´A\|$?
It turns out we can in turns of the _condition number_ of the matrix:

**Definition 2 (condition number)**
For a square matrix $A$, the _condition number_ (in $p$-norm) is
$$
Îº_p(A) := \| A \|_p \| A^{-1} \|_p
$$
with the $2$-norm:
$$
Îº_2(A) = {Ïƒ_1 \over Ïƒ_n}.
$$


**Theorem 4 (relative-error for matrix-vector)**
The _worst-case_ relative error in $A ğ± â‰ˆ (A + Î´A) ğ±$ is
$$
{\| Î´A ğ± \| \over \| A ğ± \| } â‰¤ Îº(A) Îµ
$$
if we have the relative pertubation error $\|Î´A\| = \|A \| Îµ$.

**Proof**
We can assume $A$ is invertible (as otherwise $Îº(A) = âˆ$). Denote $ğ² = A ğ±$ and we have
$$
{\|ğ± \| \over \| A ğ± \|} = {\|A^{-1} ğ² \| \over \|ğ² \|} â‰¤Â \| A^{-1}\|
$$
Thus we have:
$$
{\| Î´A ğ± \| \over \| A ğ± \| } â‰¤ \| Î´A\| \|A^{-1}\| â‰¤Â Îº(A) {\|Î´A\| \over \|A \|}
$$

âˆ


Thus for floating point arithmetic we know the error is bounded by $Îº(A) E_{n,Ïµ_{\rm m}/2}$.

If one uses QR to solve $A ğ± = ğ²$ the condition number also gives a meaningful bound on the error. 
As we have already noted, there are some matrices where PLU decompositions introduce large errors, so
in that case well-conditioning is not a guarantee (but it still usually works).
