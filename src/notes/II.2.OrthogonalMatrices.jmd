# II.2 Orthogonal and Unitary Matrices

A very important class of matrices are _orthogonal_ and _unitary_ matrices:

**Definition 1 (orthogonal/unitary matrix)** A square real matrix is _orthogonal_ if its inverse is its transpose:
$$
O(n) = \{Q âˆˆ â„^{n Ã— n} : Q^âŠ¤Q = I \}
$$
A square complex matrix is _unitary_ if its inverse is its adjoint:
$$
U(n) = \{Q âˆˆ â„‚^{n Ã— n} : Q^â‹†Q = I \}.
$$
Here the adjoint is the same as the conjugate-transpose: $Q^â‹† := QÌ„^âŠ¤$. 


Note that $O(n) âŠ‚ U(n)$ as for real matrices $Q^â‹† = Q^âŠ¤$. Because in either case $Q^{-1} = Q^â‹†$ we also have
$Q Q^â‹† = I$ (which for real matrices is $Q Q^âŠ¤ = I$). These matrices are particularly important for
numerical linear algebra for a number of reasons (we'll explore these properties in the problem sheets):

1. They are norm-preserving: for any vector $ð± âˆˆ â„‚^n$ we have
$\|Q ð± \| = \| ð±\|$ where $\| ð± \|^2 := âˆ‘_{k=1}^n x_k^2$ (i.e. the 2-norm).
2. All eigenvalues have absolute value equal to $1$
3. For $Q âˆˆ O(n)$,  $\det Q = Â±1$.
2. They are trivially invertible (just take the transpose).
3. They are generally "stable": errors are controlled.
4. They are _normal matrices_: they commute with their adjoint ($Q Q^â‹† = Q Q^â‹†$). See Chapter C for
why this is important. 

On a computer there are multiple ways of representing orthogonal/unitary matrices,
and it is almost never to store a dense matrix storing the entries. We shall therefore
investigate three classes: 

1. _Permutation_: A permutation matrix permutes the rows of a vector and is a representation of the symmetric group.
2. _Rotations_: The simple rotations are also known as $2 Ã— 2$ special orthogonal matrices ($SO(2)$)  and correspond to rotations in 2D.
3. _Reflections_:  Reflections are $n Ã— n$ orthogonal matrices that have simple definitions in terms of a single vector.

We remark a very similar concept are rectangular matrices with orthogonal columns, e.g.
$$
U = [ð®_1 | â‹¯ | ð®_n] âˆˆ â„^{m Ã— n}
$$
where $m â‰¥ n$ such that
$U^âŠ¤U =  I_n$ (the $n Ã— n$ identity matrix). In this case we must have $UU^âŠ¤ â‰  I_m$ as the rank of $U$ is $n$. These will play an important
role in the Singular Value Decomposition.

## 1. Permutation Matrices



Permutation matrices are matrices that represent the action of permuting the entries of a vector,
that is, matrix representations of the symmetric group $S_n$, acting on $â„^n$.
Recall every $Ïƒ âˆˆ S_n$ is a bijection between $\{1,2,â€¦,n\}$ and itself.
We can write a permutation $Ïƒ$ in _Cauchy notation_:
$$
\begin{pmatrix}
 1 & 2 & 3 & â‹¯ & n \cr
 Ïƒ_1 & Ïƒ_2 & Ïƒ_3 & â‹¯ & Ïƒ_n
 \end{pmatrix}
$$
where $\{Ïƒ_1,â€¦,Ïƒ_n\} = \{1,2,â€¦,n\}$ (that is, each integer appears precisely once).
We denote the _inverse permutation_ by $Ïƒ^{-1}$, which can be constructed by swapping the rows of
the Cauchy notation and reordering.

We can encode a permutation in vector $\mathbf Ïƒ = [Ïƒ_1,â€¦,Ïƒ_n]$. 
This induces an action on a vector (using indexing notation)
$$
ð¯[\mathbf Ïƒ] = \begin{bmatrix}v_{Ïƒ_1}\\ \vdots \\ v_{Ïƒ_n} \end{bmatrix}
$$


**Example 1 (permutation of a vector)** 
Consider the permutation $Ïƒ$ given by
$$
\begin{pmatrix}
 1 & 2 & 3 & 4 & 5 \cr
 1 & 4 & 2 & 5 & 3
 \end{pmatrix}
$$
We can apply it to a vector:
```julia
Ïƒ = [1, 4, 2, 5, 3]
v = [6, 7, 8, 9, 10]
v[Ïƒ] # we permutate entries of v
```
Its inverse permutation $Ïƒ^{-1}$ has Cauchy notation coming from swapping the rows of
the Cauchy notation of $Ïƒ$ and sorting:
$$
\begin{pmatrix}
 1 & 4 & 2 & 5 & 3 \cr
 1 & 2 & 3 & 4 & 5
 \end{pmatrix} \rightarrow \begin{pmatrix}
 1 & 2 & 4 & 3 & 5 \cr
 1 & 3 & 2 & 5 & 4
 \end{pmatrix} 
$$
Julia has the function `invperm` for computing the vector that encodes
the inverse permutation:
And indeed:
```julia
Ïƒâ»Â¹ = invperm(Ïƒ) # note that â»Â¹ are just unicode characters in the variable name
```
And indeed permuting the entries by `Ïƒ` and then by `Ïƒâ»Â¹` returns us
to our original vector:
```julia
v[Ïƒ][Ïƒâ»Â¹] # permuting by Ïƒ and then Ïƒâ± gets us back
```



Note that the operator
$$
P_Ïƒ(ð¯) = ð¯[\mathbf Ïƒ]
$$
is linear in $ð¯$, therefore, we can identify it with a matrix whose action is:
$$
P_Ïƒ \begin{bmatrix} v_1\\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix}v_{Ïƒ_1} \\ \vdots \\ v_{Ïƒ_n}  \end{bmatrix}.
$$
The entries of this matrix are
$$
P_Ïƒ[k,j] = ðž_k^âŠ¤ P_Ïƒ ðž_j = ðž_k^âŠ¤ ðž_{Ïƒ^{-1}_j} = Î´_{k,Ïƒ^{-1}_j} = Î´_{Ïƒ_k,j}
$$
where $Î´_{k,j}$ is the _Kronecker delta_:
$$
Î´_{k,j} := \begin{cases} 1 & k = j \\
                        0 & \hbox{otherwise}
                        \end{cases}.
$$


This construction motivates the following definition:

**Definition 2 (permutation matrix)** $P \in â„^{n Ã— n}$ is a permutation matrix if it is equal to
the identity matrix with its rows permuted.

**Example 2 (5Ã—5 permutation matrix)**
We can construct the permutation representation for $Ïƒ$ as above as follows:
```julia
P = I(5)[Ïƒ,:]
```
And indeed, we see its action is as expected:
```julia
P * v
```

**Remark (advanced)** Note that `P` is a special type `SparseMatrixCSC`. This is used
to represent a matrix by storing only the non-zero entries as well as their location.
This is an important data type in high-performance scientific computing, but we will not
be using general sparse matrices in this module.

**Proposition 1 (permutation matrix inverse)** 
Let $P_Ïƒ$ be a permutation matrix corresponding to the permutation $Ïƒ$. Then
$$
P_Ïƒ^âŠ¤ = P_{Ïƒ^{-1}} = P_Ïƒ^{-1}
$$
That is, $P_Ïƒ$ is _orthogonal_:
$$
P_Ïƒ^âŠ¤ P_Ïƒ = P_Ïƒ P_Ïƒ^âŠ¤ = I.
$$

**Proof**

We prove orthogonality via:
$$
ðž_k^âŠ¤ P_Ïƒ^âŠ¤ P_Ïƒ ðž_j = (P_Ïƒ ðž_k)^âŠ¤ P_Ïƒ ðž_j = ðž_{Ïƒ^{-1}_k}^âŠ¤ ðž_{Ïƒ^{-1}_j} = Î´_{k,j}
$$
This shows $P_Ïƒ^âŠ¤ P_Ïƒ = I$ and hence $P_Ïƒ^{-1} = P_Ïƒ^âŠ¤$. 

âˆŽ




## 2. Rotations

We begin with a general definition:

**Definition 3 (Special Orthogonal and Rotations)** _Special Orthogonal Matrices_ are
$$
SO(n) := \{Q âˆˆ O(n) | \det Q = 1 \}
$$
And (simple) _rotations_ are $SO(2)$.

In what follows we use the following for writing the angle of a vector:

**Definition 4 (two-arg arctan)** The two-argument arctan function gives the angle `Î¸` through the point
$[a,b]^âŠ¤$, i.e., 
$$
\sqrt{a^2 + b^2} \begin{bmatrix} \cos Î¸ \\ \sin Î¸ \end{bmatrix} =  \begin{bmatrix} a \\ b \end{bmatrix}
$$
It can be defined in terms of the standard arctan as follows:
$$
{\rm atan}(b,a) := \begin{cases} {\rm atan}{b \over a} & a > 0 \\
                            {\rm atan}{b \over a} + Ï€ & a < 0\hbox{ and }b >0 \\
                            {\rm atan}{b \over a} - Ï€ & a < 0\hbox{ and }b < 0 \\
                            Ï€/2 & a = 0\hbox{ and }b >0 \\
                            -Ï€/2 & a = 0\hbox{ and }b < 0 
                            \end{cases}
$$ 

This is available in Julia via the function `atan(y,x)`.


We show $SO(2)$ are exactly equivalent to standard rotations:


**Proposition 2 (simple rotation)**
A 2Ã—2 _rotation matrix_ through angle $Î¸$ is
$$
Q_Î¸ := \begin{bmatrix} \cos Î¸ & -\sin Î¸ \cr \sin Î¸ & \cos Î¸ \end{bmatrix}
$$
We have $Q âˆˆ SO(2)$ iff $Q = Q_Î¸$ for some $Î¸ âˆˆ â„$.

**Proof**

We will write $c = \cos Î¸$ and $s = \sin Î¸$. Then we have
$$
Q_Î¸^âŠ¤Q_Î¸ = \begin{pmatrix} c & s \\ -s & c \end{pmatrix} \begin{pmatrix} c & -s \\ s & c \end{pmatrix} = 
\begin{pmatrix} c^2 + s^2 & 0 \\ 0 & c^2 + s^2 \end{pmatrix} = I
$$
and $\det Q_Î¸ = c^2 + s^2 = 1$ hence $Q_Î¸ âˆˆ SO(2)$. 

Now suppose $Q = [ðª_1, ðª_2] âˆˆ SO(2)$ where we know its columns have norm 1 $\|ðª_k\| = 1$ and are orthogonal.
Write $ðª_1 = [c,s]$ where we know $c = \cos Î¸$ and $s = \sin Î¸$ for $Î¸ = {\rm atan}(s, c)$. 
Since $ðª_1\cdot ðª_2 = 0$ we can deduce $ðª_2 = Â± [-s,c]$. The sign is positive as $\det Q = Â±(c^2 + s^2) = Â±1$.

âˆŽ




We can rotate an arbitrary vector in $â„^2$ to the unit axis using rotations, which are useful in
linear algebra decompositions. Interestingly it only requires
basic algebraic functions (no trigonometric functions):



**Proposition 3 (rotation of a vector)** 
The matrix
$$Q = {1 \over \sqrt{a^2 + b^2}}\begin{bmatrix}
 a & b \cr -b & a
\end{bmatrix}
$$
is a rotation matrix ($Q âˆˆ SO(2)$) satisfying
$$
Q \begin{bmatrix} a \\ b \end{bmatrix} = \sqrt{a^2 + b^2} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

**Proof** 

The last equation is trivial so the only question is that it is a rotation matrix. This follows immediately:
$$
Q^âŠ¤ Q = {1 \over a^2 + b^2}  \begin{bmatrix}
 a^2 + b^2 & 0 \cr 0 & a^2 + b^2
\end{bmatrix} = I
$$
and $\det Q = 1$.

âˆŽ




## 3. Reflections

In addition to rotations, another type of orthogonal/unitary matrix are reflections:

**Definition 5 (reflection matrix)** 
Given a unit vector $ð¯ âˆˆ â„‚^n$ (satisfying $\|ð¯\|=1$), the _reflection matrix_
$$
Q_ð¯ := I - 2 ð¯ ð¯^â‹†
$$


These are reflections in the direction of $ð¯$. We can show this as follows:

**Proposition 4 (Householder properties)** $Q_ð¯$ satisfies:
1. $Q_ð¯ = Q_ð¯^â‹†$ (Symmetry)
2. $Q_ð¯^â‹† Q_ð¯ = I$ (Orthogonality $Q_ð¯ âˆˆ U(n)$)
2. $ð¯$ is an eigenvector of $Q_ð¯$ with eigenvalue $-1$
4. $Q_ð¯$ is a rank-1 perturbation of $I$
3. $\det Q_ð¯ = -1$ ($Q_ð¯ âˆ‰ SO(n)$)


**Proof**

Property 1 follows immediately. Property 2 follows from
$$
Q_ð¯^â‹† Q_ð¯ = Q_ð¯^2 = I - 4 ð¯ ð¯^â‹† + 4 ð¯ ð¯^â‹† ð¯ ð¯^â‹† = I
$$
Property 3 follows since
$$
Q_ð¯ ð¯ = -ð¯
$$
Property 4 follows since $ð¯ ð¯^âŠ¤$ is a rank-1 matrix as all rows are linear combinations of each other.
To see property 5, note there is a dimension $n-1$ space $W$ orthogonal to $ð¯$, that is, for all
$ð° âˆˆ W$ we have $ð°^â‹† ð¯ = 0$, which implies that
$$
Q_ð¯ ð° = ð°
$$
In other words, $1$ is an eigenvalue with multiplicity $n-1$ and $-1$ is an eigenvalue with multiplicity 1,
and thus the product of the eigenvalues is $-1$.

âˆŽ



**Example 3 (reflection through 2-vector)** Consider reflection through $ð± = [1,2]^âŠ¤$. 
We first need to normalise $ð±$:
$$
ð¯ = {ð± \over \|ð±\|} = \begin{bmatrix} {1 \over \sqrt{5}} \\ {2 \over \sqrt{5}} \end{bmatrix}
$$
Note this indeed has unit norm:
$$
\|ð¯\|^2 = {1 \over 5} + {4 \over 5} = 1.
$$
Thus the reflection matrix is:
$$
Q_ð¯ = I - 2 ð¯ ð¯^âŠ¤ = \begin{bmatrix}1 \\ & 1 \end{bmatrix} - {2 \over 5} \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}
 =  {1 \over 5} \begin{bmatrix} 3 & -4 \\ -4 & -3 \end{bmatrix}
$$
Indeed it is symmetric, and orthogonal. It sends $ð±$ to $-ð±$:
$$
Q_ð¯ ð± = {1 \over 5} \begin{bmatrix}3 - 8 \\ -4 - 6 \end{bmatrix} = -ð±
$$
Any vector orthogonal to $ð±$, like $ð² = [-2,1]^âŠ¤$, is left fixed:
$$
Q_ð¯ ð² = {1 \over 5} \begin{bmatrix}-6 -4 \\ 8 - 3 \end{bmatrix} = ð²
$$


Note that _building_ the matrix $Q_ð¯$ will be expensive ($O(n^2)$ operations), but we can _apply_
$Q_ð¯$ to a vector in $O(n)$ operations using the expression:
$$
Q_ð¯ ð± = ð± - 2 ð¯ (ð¯^â‹† ð±) = ð± - 2 ð¯ (ð¯ â‹… ð±).
$$


Just as rotations can be used to rotate vectors to be aligned with coordinate axis, so can reflections,
but in this case it works for vectors in $â„‚^n$, not just $â„^2$:

**Definition 6 (Householder reflection, real case)** For a given vector
$ð± âˆˆ â„^n$, define the Householder reflection
$$
Q_ð±^{Â±,\rm H} := Q_ð°
$$
for $ð² = âˆ“ \|ð±\| ðž_1 + ð±$ and $ð° = {ð² \over \|ð²\|}$.
The default choice in sign is:
$$
Q_ð±^{\rm H} := Q_ð±^{-\hbox{sign}(x_1),\rm H}.
$$


**Lemma 1 (Householder reflection maps to axis)** For $ð± âˆˆ â„^n$,
$$
Q_ð±^{Â±,\rm H} ð± = Â±\|ð±\| ðž_1
$$

**Proof**
Note that
$$
\begin{align*}
\| ð² \|^2 &= 2\|ð±\|^2 âˆ“ 2 \|ð±\| x_1, \\
ð²^âŠ¤ ð± &= \|ð±\|^2 âˆ“  \|ð±\| x_1
\end{align*}
$$
where $x_1 = ðž_1^âŠ¤ ð±$. Therefore:
$$
Q_ð±^{Â±,\rm H} ð±  =  (I - 2 ð° ð°^âŠ¤) ð± = ð± - 2 {ð²  \|ð±\|  \over \|ð²\|^2} (\|ð±\|âˆ“x_1) = ð± - ð² =  Â±\|ð±\| ðž_1.
$$

âˆŽ

Why do we choose the the opposite sign of $x_1$ for the default reflection? For stability.
We demonstrate the reason for this by numerical example. Consider $ð± = [1,h]$, i.e., a small perturbation
from $ðž_1$. If we reflect to $\hbox{norm}(ð±)ðž_1$ we see a numerical problem:
```julia
h = 10.0^(-10)
x = [1,h]
y = -norm(x)*[1,0] + x
w = y/norm(y)
Q = I - 2w*w'
Q*x
```
It didn't work! Even worse is if `h = 0`:
```julia
h = 0
x = [1,h]
y = -norm(x)*[1,0] + x
w = y/norm(y)
Q = I - 2w*w'
Q*x
```
This is because `y` has large relative error due to cancellation
from floating point errors in computing the first entry `x[1] - norm(x)`. 
(Or has norm zero if `h=0`.)
We avoid this cancellation by using the default choice:
```julia
h = 10.0^(-10)
x = [1,h]
y = sign(x[1])*norm(x)*[1,0] + x
w = y/norm(y)
Q = I - 2w*w'
Q*x
```

We can extend this definition for complexes:

**Definition 7 (Householder reflection, complex case)** For a given vector
$ð± âˆˆ â„‚^n$, define the Householder reflection as
$$
Q_ð±^{\rm H} := Q_ð°
$$
for $ð² = {\rm csign}(x_1) \|ð±\| ðž_1 + ð±$ and $ð° = {ð² \over \|ð²\|}$, for ${\rm csign}(z) = {\rm e}^{{\rm i} \arg z}$. 


**Lemma 2 (Householder reflection maps to axis, complex case)** For $ð± âˆˆ â„‚^n$,
$$
Q_ð±^{\rm H} ð± = -{\rm csign}(x_1) \|ð±\| ðž_1
$$

**Proof**
Denote $Î± := {\rm csign}(x_1)$. 
Note that $Î±Ì„ x_1 = {\rm e}^{-{\rm i} \arg x_1} x_1 = |x_1|$.  Now we have
$$
\begin{align*}
\| ð² \|^2 &= (Î± \|ð±\| ðž_1 + ð±)^â‹†(Î± \|ð±\| ðž_1 + ð±) = |Î±|\| ð± \|^2 + \| ð± \|  Î± xÌ„_1 + Î±Ì„ x_1 \| ð± \| + \| ð± \|^2 \\
&= 2\| ð± \|^2 + 2|x_1| \| ð± \| \\
ð²^â‹† ð± &= Î±Ì„ x_1 \| ð± \| + \|ð± \|^2 = \|ð± \|^2 + |x_1| \| ð± \|
\end{align*}
$$
Therefore:
$$
Q_ð±^{\rm H} ð±  =  (I - 2 ð° ð°^â‹†) ð± = ð± - 2 {ð²    \over \|ð²\|^2} (\|ð± \|^2 + |x_1| \|ð± \|) = ð± - ð² =  -Î± \|ð±\| ðž_1.
$$

âˆŽ