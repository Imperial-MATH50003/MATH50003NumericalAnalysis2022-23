# III.6 Gaussian quadrature


In this chapter we see that a special quadrature rule can be constructed by using the roots of orthogonal
polynomials, leading to a method that is exact for polynomials of twice the expected degree.
Importantly, we can use quadrature to compute expansions in orthogonal polynomials that
interpolate,  mirroring the link between the Trapezium rule, Fourier series, and interpolation but
now for orthogonal polynomials.

1. Truncated Jacobi matrices: we see that truncated Jacobi matrices are diagonalisable
in terms of orthogonal polynomials and their zeros. 
2. Gaussian quadrature: Using roots of orthogonal polynomials and truncated Jacobi matrices 
leads naturally to an efficiently
computable interpolatory quadrature rule. The _miracle_ is its exact for twice as many polynomials as
expected.




## 1. Roots of orthogonal polynomials and truncated Jacobi matrices

We now consider roots (zeros) of orthogonal polynomials $p_n(x)$. This is important as we shall
see they are useful for interpolation and quadrature. For interpolation to be well-defined we
first need to guarantee that the roots are distinct.

**Lemma 1** An orthogonal polynomial $p_n(x)$ has exactly $n$ distinct roots.

**Proof**

Suppose $x_1, â€¦,x_j$ are the roots where $q_n(x)$ changes sign, that is,
$$
p_n(x) = c_k (x-x_k)^{2p+1} + O((x-x_k)^{2p+2})
$$
for $c_k â‰  0$ and $k = 1,â€¦,j$ and $p âˆˆ â„¤$, as $x â†’ x_k$. Then
$$
p_n(x) (x-x_1) â‹¯(x-x_j)
$$
does not change signs: it behaves like $c_k (x-x_k)^{2p+2} + O(x-x_k)^{2p+3}$ as $x â†’ x_k$.
In other words:
$$
âŸ¨p_n,(x-x_1) â‹¯(x-x_j) âŸ© = \int_a^b p_n(x) (x-x_1) â‹¯(x-x_j) w(x) {\rm d} x â‰  0.
$$
where $w(x)$ is the weight of orthogonality.
This is only possible if $j = n$ as $p_n(x)$ is orthogonal w.r.t. all lower degree
polynomials.

âˆ

**Definition 1 (truncated Jacobi matrix)** Given a symmetric Jacobi matrix $X$
(associated with a family of orthonormal polynomials),
 the _truncated Jacobi matrix_ is
$$
J_n := \begin{bmatrix} a_0 & b_0 \\
                         b_0 & â‹± & â‹± \\
                         & â‹± & a_{n-2} & b_{n-2} \\
                         && b_{n-2} & a_{n-1} \end{bmatrix} âˆˆ â„^{n Ã— n}
$$



**Lemma 2 (zeros)** The zeros $x_1, â€¦,x_n$ of an orthonormal polynomial $q_n(x)$
are the eigenvalues of the truncated Jacobi matrix $J_n$.
More precisely,
$$
J_n Q_n = Q_n \begin{bmatrix} x_1 \\ & â‹± \\ && x_n \end{bmatrix}
$$
for the orthogonal matrix
$$
Q_n = \begin{bmatrix}
q_0(x_1) & â‹¯ & q_0(x_n) \\
â‹®  & â‹¯ & â‹®  \\
q_{n-1}(x_1) & â‹¯ & q_{n-1}(x_n)
\end{bmatrix} \begin{bmatrix} Î±_1^{-1} \\ & â‹± \\ && Î±_n^{-1} \end{bmatrix}
$$
where $Î±_j = \sqrt{q_0(x_j)^2 + â‹¯ + q_{n-1}(x_j)^2}$.

**Proof**

We construct the eigenvector (noting $b_{n-1} q_n(x_j) = 0$):
$$
J_n \begin{bmatrix} q_0(x_j) \\ â‹® \\ q_{n-1}(x_j) \end{bmatrix} =
\begin{bmatrix} a_0 q_0(x_j) + b_0 q_1(x_j) \\
 b_0 q_0(x_j) + a_1 q_1(x_j) + b_1 q_2(x_j) \\
â‹® \\
b_{n-3} q_{n-3}(x_j) + a_{n-2} q_{n-2}(x_j) + b_{n-2} q_{n-1}(x_j) \\
b_{n-2} q_{n-2}(x_j) + a_{n-1} q_{n-1}(x_j) + b_{n-1} q_n(x_j)
\end{bmatrix} = x_j \begin{bmatrix} q_0(x_j) \\
 q_1(x_j) \\
â‹® \\
q_{n-1}(x_j)
\end{bmatrix}
$$
The result follows from normalising the eigenvectors. Since $J_n$ is symmetric
the eigenvector matrix is orthogonal.

âˆ

**Example 1 (Chebyshev roots)** Consider $T_n(x) = \cos n {\rm acos}\, x$. The roots 
are $x_j = \cos Î¸_j$ where $Î¸_j = (j-1/2)Ï€/n$ for $j = 1,â€¦,n$ are the roots of $\cos n Î¸$
that are inside $[0,Ï€]$. 

Consider the $n = 3$ case where we have
$$
x_1,x_2,x_3 = \cos(Ï€/6),\cos(Ï€/2),\cos(5Ï€/6) = \sqrt{3}/2,0,-\sqrt{3/2}
$$
We also have from the 3-term recurrence:
$$
\begin{align*}
T_0(x) = 1 \\
T_1(x) = x \\
T_2(x) = 2x T_1(x) - T_0(x) = 2x^2-1 \\
T_3(x) = 2x T_2(x) - T_1(x) = 4x^3-3x
\end{align*}
$$
We orthonormalise by rescaling
$$
\begin{align*}
q_0(x) &= 1/\sqrt{Ï€} \\
q_k(x) &= T_k(x) \sqrt{2}/\sqrt{Ï€}
\end{align*}
$$
so that the Jacobi matrix is symmetric:
$$
x [q_0(x)|q_1(x)|â‹¯] = [q_0(x)|q_1(x)|â‹¯] \underbrace{\begin{bmatrix} 0 & 1/\sqrt{2} \\
                            1/\sqrt{2} & 0 & 1/2 \\
                            &1/2 & 0 & 1/2 \\
                             &   & 1/2 & 0 & â‹± \\
                              &  && â‹± & â‹±
\end{bmatrix}}_X
$$
We can then confirm that we have constructed an eigenvector/eigenvalue of the $3 Ã— 3$ truncation of the Jacobi matrix,
e.g. at $x_2 = 0$:
$$
\begin{bmatrix} 
0 & 1/\sqrt{2} \\
1/\sqrt{2} & 0 & 1/2 \\
    & 1/2 & 0\end{bmatrix} \begin{bmatrix} q_0(0) \\ q_1(0) \\ q_2(0) 
    \end{bmatrix} = {1 \over \sqrt Ï€} \begin{bmatrix} 
0 & 1/\sqrt{2} \\
1/\sqrt{2} & 0 & 1/2 \\
    & 1/2 & 0\end{bmatrix} \begin{bmatrix} 1 \\ 0 \\ -{1 \over \sqrt{2}}
    \end{bmatrix} =\begin{bmatrix} 0 \\ 0 \\ 0
    \end{bmatrix}
$$

## 2. Gaussian quadrature

Gaussian quadrature is the interpolatory quadrature rule corresponding
to the grid $x_j$ defined as the roots of the orthonormal polynomial $q_n(x)$.
We shall see that it is exact for polynomials up to degree $2n-1$, i.e., double
the degree of other interpolatory quadrature rules from other grids.



**Definition 2 (Gauss quadrature)** Given a weight $w(x)$, the Gauss quadrature rule is:
$$
âˆ«_a^b f(x)w(x) {\rm d}x â‰ˆ \underbrace{âˆ‘_{j=1}^n w_j f(x_j)}_{Î£_n^w[f]}
$$
where $x_1,â€¦,x_n$ are the roots of the orthonormal polynomials $q_n(x)$ and 
$$
w_j := {1 \over Î±_j^2} = {1 \over q_0(x_j)^2 + â‹¯ + q_{n-1}(x_j)^2}.
$$
Equivalentally, $x_1,â€¦,x_n$ are the eigenvalues of $J_n$ and
$$
w_j = âˆ«_a^b w(x) {\rm d}x Q_n[1,j]^2.
$$
(Note we have $âˆ«_a^b w(x) {\rm d} x q_0(x)^2 = 1$.)

In analogy to how Fourier series are orthogonal with respect to the Trapezium rule,
Orthogonal polynomials are orthogonal with respect to Gaussian quadrature:

**Lemma 3 (Discrete orthogonality)**
For $0 â‰¤Â â„“,m â‰¤Â n-1$, the orthonormal polynomials $q_n(x)$ satisfy
$$
Î£_n^w[q_â„“ q_m] = Î´_{â„“m}
$$

**Proof**
$$
Î£_n^w[q_â„“ q_m] = âˆ‘_{j=1}^n {q_â„“(x_j) q_m(x_j) \over Î±_j^2}
= \left[q_â„“(x_1)/ Î±_1 | â‹¯ | {q_â„“(x_n)/ Î±_n}\right] 
\begin{bmatrix}
q_m(x_1)/Î±_1 \\
â‹® \\
q_m(x_n)/Î±_n \end{bmatrix} = ğ_â„“ Q_n Q_n^âŠ¤ ğ_m = Î´_{â„“m}
$$

âˆ

Just as approximating Fourier coefficients using Trapezium rule gives a way of
interpolating at the grid, so does Gaussian quadrature:

**Theorem 1 (interpolation via quadrature)**
For the orthonormal polynomials $q_n(x)$,
$$
f_n(x) := âˆ‘_{k=0}^{n-1} c_k^n q_k(x)\hbox{ for } c_k^n := Î£_n^w[f q_k]
$$
interpolates $f(x)$ at the Gaussian quadrature points $x_1,â€¦,x_n$.

**Proof**

Consider the Vandermonde-like matrix:
$$
VÌƒ := \begin{bmatrix} q_0(x_1) & â‹¯ & q_{n-1}(x_1) \\
                â‹® & â‹± & â‹® \\
                q_0(x_n) & â‹¯ & q_{n-1}(x_n) \end{bmatrix}
$$
and define
$$
Q_n^w := VÌƒ^âŠ¤ \begin{bmatrix} w_1 \\ &â‹± \\&& w_n \end{bmatrix} = \begin{bmatrix} q_0(x_1)w_1 & â‹¯ &  q_0(x_n) w_n \\
                â‹® & â‹± & â‹® \\
                w_1q_{n-1}(x_1) & â‹¯ & q_{n-1}(x_n)w_n \end{bmatrix}
$$
so that
$$
\begin{bmatrix}
c_0^n \\
â‹® \\
c_{n-1}^n \end{bmatrix} = Q_n^w \begin{bmatrix} f(x_1) \\ â‹® \\ f(x_n) \end{bmatrix}.
$$
Note that if $p(x) = [q_0(x) | â‹¯ | q_{n-1}(x)] ğœ$ then
$$
\begin{bmatrix}
p(x_1) \\
â‹® \\
p(x_n)
\end{bmatrix} = VÌƒ ğœ
$$
But we see that (similar to the Fourier case)
$$
Q_n^w VÌƒ = \begin{bmatrix} Î£_n^w[q_0 q_0] & â‹¯ & Î£_n^w[q_0 q_{n-1}]\\
                â‹® & â‹± & â‹® \\
                Î£_n^w[q_{n-1} q_0] & â‹¯ & Î£_n^w[q_{n-1} q_{n-1}]
                \end{bmatrix} = I_n
$$

âˆ


**Example 2 (Chebyshev expansions)** 
Consider the construction of Gaussian quadrature associated with the Chebyshev weight for $n = 3$. 
To determine the weights we need:
$$
w_j^{-1} = Î±_j^2 = q_0(x_j)^2 + q_1(x_j)^2 + q_2(x_j)^2 = 
{1 \over Ï€} + {2 \over Ï€} x_j^2 + {2 \over Ï€} (2x_j^2-1)^2
$$
We can check each case and deduce that $w_j = Ï€/3$.
Thus we recover the interpolatory quadrature rule.
Further, we can construct the transform
$$
\begin{align*}
Q_3^w &= \begin{bmatrix}
w_1 q_0(x_1) & w_2 q_0(x_2) & w_3 q_0(x_3) \\
w_1 q_1(x_1) & w_2 q_1(x_2) & w_3 q_1(x_3) \\
w_1 q_3(x_1) & w_2 q_3(x_2) & w_3 q_3(x_3) 
\end{bmatrix}\\
&= {Ï€ \over 3} \begin{bmatrix} 1/\sqrt{Ï€} & 1/\sqrt{Ï€} & 1/\sqrt{Ï€} \\
                                x_1\sqrt{2/Ï€} & x_2\sqrt{2/Ï€} & x_3\sqrt{2/Ï€} \\
                                (2x_1^2-1)\sqrt{2/Ï€} &(2x_2^2-1)\sqrt{2/Ï€} & (2x_3^2-1)\sqrt{2/Ï€}
                                \end{bmatrix} \\
                                &= 
                                {\sqrt{Ï€} \over 3} \begin{bmatrix} 1 & 1 & 1 \\
                                \sqrt{6}/2 & 0 & -\sqrt{6}/2 \\
                                1/\sqrt{2} &-\sqrt{2} & 1/\sqrt{2}
                                \end{bmatrix}
\end{align*}
$$
We can use this to expand a polynomial, e.g. $x^2$:
$$
Q_3^w \begin{bmatrix}
x_1^2 \\
x_2^2 \\
x_3^2 
\end{bmatrix} = {\sqrt{Ï€} \over 3} 
\begin{bmatrix} 1 & 1 & 1 \\
\sqrt{6}/2 & 0 & -\sqrt{6}/2 \\
1/\sqrt{2} &-\sqrt{2} & 1/\sqrt{2}
\end{bmatrix} 
\begin{bmatrix} 3/4 \\ 0 \\ 3/4 \end{bmatrix} =
\begin{bmatrix}
{\sqrt{Ï€} / 2} \\
0 \\
{\sqrt{Ï€} / (2\sqrt{2})}
\end{bmatrix}
$$
In other words:
$$
x^2 = {\sqrt Ï€ \over 2} q_0(x) + {\sqrt Ï€ \over 2\sqrt 2} q_2(x) = {1 \over 2} T_0(x) + {1 \over 2} T_2(x)
$$
which can be easily confirmed.




**Corollary 1** Gaussian quadrature is an interpolatory quadrature rule
with the interpolation points equal to the roots of $q_n$:
$$
Î£_n^w[f] = âˆ«_a^b f_n(x) w(x) {\rm d}x 
$$
**Proof**
We want to show that its the same as integrating the interpolatory polynomial:
$$
\int_a^b f_n(x) w(x) {\rm d}x = {1 \over q_0(x)} \sum_{k=0}^{n-1} c_k^n \int_a^b q_k(x) q_0(x) w(x) {\rm d}x
= {c_0^n \over q_0} = Î£_n^w[f].
$$
âˆ

**Example 3 (Chebyshev quadrature via Lagrange polynomials)** The connection with interpolatory quadrature
means theres another way to compute the quadrature: integrate the Lagrange polynomials associated with
the zeros. For example, with $n = 3$ recall that the roots of $T_3(x)$ are $Â±\sqrt{3}/2$ and $0$. Thus we have
$$
\begin{align*}
â„“_1(x) &= x (x +\sqrt{3}/2) / (3/2) = {(2x^2+\sqrt{3}x) \over 3} \\
â„“_2(x) &= (x - \sqrt{3}/2) (x +\sqrt{3}/2) / (-3/4) = -{4 \over 3} x^2 + 1 \\
â„“_3(x) &=(x -\sqrt{3}/2) x / (3/2) = {(2x^2-\sqrt{3}x) \over 3}
\end{align*}
$$
A quick check confirms that
$$
w_j = âˆ«_{-1}^1 {â„“_j(x) \over \sqrt{1-x^2}} {\rm d}x = {Ï€ \over 3}.
$$





A consequence of being an interpolatory quadrature rule is that it is exact for all
polynomials of degree $n-1$. The _miracle_ of Gaussian quadrature is it is exact for twice
as many!



**Theorem 2 (Exactness of Gauss quadrature)** If $p(x)$ is a degree $2n-1$ polynomial then
Gauss quadrature is exact:
$$
âˆ«_a^b p(x)w(x) {\rm d}x = Î£_n^w[p].
$$

**Proof**
Using polynomial division algorithm (e.g. by matching terms) we can write
$$
p(x) = q_n(x) s(x) + r(x)
$$
where $s$ and $r$ are degree $n-1$ and $q_n(x)$ is the degree $n$ orthonormal polynomial. Then we have:
$$
\begin{align*}
Î£_n^w[p] &= \underbrace{Î£_n^w[q_n s]}_{\hbox{$0$ since evaluating $q_n$ at zeros}} + Î£_n^w[r] = âˆ«_a^b r(x) w(x) {\rm d}x\\
&= \underbrace{âˆ«_a^b q_n(x)s(x) w(x) {\rm d}x}_{\hbox{$0$ since $s$ is degree $<n$}}  + âˆ«_a^b r(x) w(x) {\rm d}x \\
&= âˆ«_a^b p(x)w(x) {\rm d}x.
\end{align*}
$$
âˆ


**Example 4 (Double exactness)**
We are exact for all polynomials of degree $2n-1$, so for our $n = 3$ rule consider integrating
$x^5$. We correctly get:
$$
Î£_n^w[x^5] = {Ï€ \over 3} \left( {9 \sqrt{3} \over 32}  - {9 \sqrt{3} \over 2} \right) = 0.
$$
We are also correct for $x^4$:
$$
Î£_n^w[x^4] = {Ï€ \over 3} \left( {9 \over 16} + {9 \over 16} \right) = {3 Ï€ \over 8}.
$$
However,
$$
Î£_n^w[x^6] = {9 Ï€ \over 64} â‰  {5 Ï€ \over 16}
$$
hence it is incorrect for larger degree polynomials.

