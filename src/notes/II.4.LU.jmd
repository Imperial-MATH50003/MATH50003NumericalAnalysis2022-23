# II.4 PLU and Cholesky factorisations

In this chapter we consider the following factorisations for square real invetible  matrices $A âˆˆ â„^{n Ã— n}$:
1. The _LU factorisation_:
$ A = LU$ where $L$ is lower triangular and $U$ is upper triangular. This is equivalent to Gaussian elimination without pivoting,
so may not exist (e.g. if $A[1,1] = 0$).
1. The _PLU factorisation_:
$
A = P^âŠ¤ LU
$
where $P$ is a permutation matrix, $L$ is lower triangular and $U$ is upper triangular. This is equivalent to Gaussian elimination with pivoting.
It always exists but may in extremely rare cases be unstable. 
2. For a square, _symmetric positive definite_ ($ğ±^âŠ¤ A ğ± > 0$ for all $ğ± âˆˆ â„^n$, $ğ± â‰  0$) 
matrix the LU decompostion has a special form which is called the _Cholesky factorisation_:
$
A = L L^âŠ¤
$.
3. We also discuss timing and stability of the different factorisations.

```julia
using LinearAlgebra, Plots, BenchmarkTools
```

## 1. LU Factorisation

Just as Gramâ€“Schmidt can be reinterpreted as a reduced QR factorisation,
Gaussian elimination  can be interpreted as an LU factorisation.



Consider the following set of $n Ã— n$ lower triangular
matrices which equals identity apart from one-column:
$$
{\cal L}_j := \left\{I + \begin{bmatrix} ğŸ_j \\ ğ¥_j \end{bmatrix} ğ_j^âŠ¤ : ğ¥_j âˆˆ â„^{n-j} \right\}
$$
where  $ğŸ_j$ denotes the zero vector of length $j$. 
That is, if $L_j âˆˆ {\cal L}_j$ then it is equal to the identity matrix apart from in the $j$ th column:
$$
L_j = \begin{bmatrix}
        1 \\ & {â‹±} \\ && 1 \\
                    && â„“_{j+1,j} & 1 \\
                    && â‹® && \dots \\
                    && â„“_{n,j} & & & 1 \end{bmatrix} = 
$$

These satisify the following special properties:

**Proposition 1 (one-column lower triangular inverse)**
If $L_j âˆˆ {\cal L}_j$ then
$$
L_j^{-1}  = I - \begin{bmatrix} ğŸ_j \\ ğ¥_j \end{bmatrix} ğ_j^âŠ¤ = \begin{bmatrix}
        1 \\ & â‹± \\ && 1 \\
                    &&-â„“_{j+1,j} & 1 \\
                    &&â‹® && \dots \\
                    &&-â„“_{n,j} & & & 1 \end{bmatrix} âˆˆ {\cal L}_j.
$$


**Proposition 2 (one-column lower triangular multiplication)**
If $L_j âˆˆ {\cal L}_j$ and $L_k âˆˆ {\cal L}_k$ for $k â‰¥ j$ then
$$
L_j L_k =  I + \begin{bmatrix} ğŸ_j \\ ğ¥_j \end{bmatrix} ğ_j^âŠ¤ +  \begin{bmatrix} ğŸ_k \\ ğ¥_k \end{bmatrix} ğ_k^âŠ¤
$$


**Lemma 1 (one-column lower triangular with pivoting)**
If $Ïƒ$ is a permutation that leaves the first $j$
rows fixed (that is, $Ïƒ_â„“ = â„“$ for $â„“ â‰¤Â j$) and $L_j âˆˆ {\cal L}_j$ then
$$
P_Ïƒ L_j=  \tilde L_j P_Ïƒ
$$
where $\tilde L_j âˆˆ {\cal L}_j$.

**Proof**
Write
$$
P_Ïƒ = \begin{bmatrix} I_j \\ & P_Ï„ \end{bmatrix}
$$
where $Ï„$ is the permutation with Cauchy notation
$$
\begin{pmatrix}
1 & â‹¯ & n-j \\
Ïƒ_{j+1}-j & â‹¯ & Ïƒ_n-j
\end{pmatrix}
$$
Then we have
$$
P_Ïƒ L_j = P_Ïƒ + \begin{bmatrix} ğŸ_j \\ P_Ï„ ğ¥_j \end{bmatrix} ğ_j^âŠ¤ =
\underbrace{(I +  \begin{bmatrix} ğŸ_j \\ P_Ï„ ğ¥_j \end{bmatrix} ğ_j^âŠ¤)}_{\tilde L_j} P_Ïƒ
$$
noting that $ğ_j^âŠ¤ P_Ïƒ = ğ_j^âŠ¤$ (as $Ïƒ_j = j$). 
âˆ


Now consider standard Gaussian elimation where one row-reduces
to introduce zeros column-by-column. We will mimick the computation of the QR factorisation
to view this as a _triangular triangularisation_.


Consider the matrix
$$
L_1 = \begin{bmatrix} 1 \\ -{a_{21} \over a_{11}} & 1 \\ â‹® &&â‹± \\
                -{a_{n1} \over a_{11}}  &&& 1
                \end{bmatrix} = I - \begin{bmatrix} 0 \\ {ğš_1[2:n] \over ğš_1[1]} \end{bmatrix}  ğ_1^âŠ¤.
$$
We then have
$$
L_1 A =  \begin{bmatrix} u_{11} & u_{12} & â‹¯ & u_{1n} \\ 
& ğš_2^1 & â‹¯ & ğš_n^1   \end{bmatrix}
$$
where $ğš_j^1 := (L_1 ğš_j)[2:n]$ and $u_{1j} = a_{1j}$. But now consider
$$
L_2 := I - \begin{bmatrix} 0 \\ {ğš_2^1[2:n-1] \over ğš_2^1[1]} \end{bmatrix}  ğ_1^âŠ¤.
$$
Then
$$
L_2 L_1 A = \begin{bmatrix} u_{11} & u_{12} & u_{13} & â‹¯ & u_{1n} \\ 
    & u_{22} & u_{23} & â‹¯ & u_{2n} \\
&& ğš_3^2 & â‹¯ & ğš_n^2   \end{bmatrix}
$$
where 
$$
u_{2j} :=  (ğš_j^1)[1] \qquad \hbox{and} \qquad ğš_j^2 := (L_2 ğš_j^1)[2:n-1]
$$
Thus the first two columns are triangular. 

The inductive construction is again clear. If we define $ğš_j^0 := ğš_j$ we
have the construction
$$
\begin{align*}
L_j &:= I - \begin{bmatrix} ğŸ_j \\ {ğš_{j+1}^j[2:n-j] \over ğš_{j+1}^j[1]} \end{bmatrix} ğ_j^âŠ¤ \\
ğš_j^k &:= (L_k ğš_j^{k-1})[2:n-k+1]
 \\
u_{kj} &:= (L_k ğš_j^{k-1})[1]
\end{align*}
$$
Then
$$
L_{n-1} â‹¯ L_1 A = \underbrace{\begin{bmatrix} 
u_{11} & â‹¯ & u_{1n} \\ & â‹± & â‹®\\
                                        && u_{nn}\end{bmatrix}}_U
$$
i.e.
$$
A = \underbrace{L_{1}^{-1} â‹¯ L_{n-1}^{-1}}_L U.
$$

Writing
$$
L_j = I + \begin{bmatrix} ğŸ_j \\ \ell_{j+1,j} \\ â‹® \\ \ell_{n,j} \end{bmatrix} ğ_j^âŠ¤
$$
and using the properties of inversion and multiplication we therefore deduce
$$
L = \begin{bmatrix} 1 \\ -\ell_{21} & 1 \\
-\ell_{31} & -\ell_{32} & 1 \\
 â‹® & â‹® & â‹± & â‹± \\
    -\ell_{n1} & -\ell_{n2} & â‹¯ & -\ell_{n,n-1} & 1
    \end{bmatrix}
$$




**Example 1 (computer)**
We will do a numerical example (by-hand is equivalent to Gaussian elimination).
The first lower triangular matrix is:
```julia
n = 4
A = randn(n,n)
Lâ‚ = I -[0; A[2:end,1]/A[1,1]] * [1 zeros(1,n-1)]
```
Which indeed introduces zeros in the first column:
```julia
Aâ‚ = Lâ‚*A
```
Now we make the next lower triangular operator:
```julia
Lâ‚‚ = I - [0; 0; Aâ‚[3:end,2]/Aâ‚[2,2]] * [0 1 zeros(1,n-2)]
```
So that
```julia
Aâ‚‚ = Lâ‚‚*Lâ‚*A
```
The last one is:
```julia
Lâ‚ƒ = I - [0; 0; 0; Aâ‚‚[4:end,3]/Aâ‚‚[3,3]] * [0 0 1 zeros(1,n-3)]
```
Giving us
```julia
U = Lâ‚ƒ*Lâ‚‚*Lâ‚*A
```
and
```julia
L = inv(Lâ‚)*inv(Lâ‚‚)*inv(Lâ‚ƒ)
```
Note how the entries of `L` are indeed identical to the negative
lower entries of `Lâ‚`, `Lâ‚‚` and `Lâ‚ƒ`.

**Example 2 (by-hand)**

Consider the matrix
$$
A = \begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix}
$$
We have
$$
L_2 L_1 A = L_2 \begin{bmatrix}1 \\ 
                        -2 & 1 \\ -1 &  & 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix}
    = \begin{bmatrix}1 \\ & 1\\ & -{3 \over 2} & 1 
    \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\
                         & 2 & 6 \\
                         & 3 & 8 \end{bmatrix}
    = \underbrace{\begin{bmatrix} 1 & 1 & 1 \\
                         & 2 & 6 \\
                         &  & -1 \end{bmatrix}}_U
$$
We then deduce $L$ by taking the negative of the lower 
entries of $L_1,L_2$:
$$
L = \begin{bmatrix} 1 \\ 2 & 1 \\ 1 &{3 \over 2} & 1
\end{bmatrix}.
$$


## 2. PLU Factorisation

We learned in first year linear algebra that if a diagonal entry is zero
whe1 doing Gaussian elimnation one has to _row pivot_. For stability, 
in implementation one _always_ pivots: swap the largest in magnitude entry for the entry on the diagonal.
In terms of a factorisation, this leads to 
$$
L_{n-1} P_{n-1} â‹¯ P_2 L_1 P_1 A = U
$$
where $P_j$ is a permutation that leaves rows 1 through $j-1$ fixed,
and swaps row $j$ with a row $k \geq j$ whose entry is maximal in absolute value.

Thus we can deduce that 
$$
L_{n-1} P_{n-1} â‹¯ P_2 L_1 P_1 = \underbrace{L_{n-1} \tilde L_{n-2} â‹¯  \tilde L_1}_{L^{-1}}  \underbrace{P_{n-1} â‹¯ P_2 P_1}_P.
$$
where the tilde denotes the combined actions of swapping the permutation and lower-triangular matrices, that is,
$$
P_{n-1}â‹¯ P_{j+1} L_j = \tilde L_j P_{n-1}â‹¯ P_{j+1}.
$$
where $\tilde L_j âˆˆ {\cal L}_j$.
The entries of $L$ are then again the negative of the entries below the diagonal of $L_{n-1}, \tilde L_{n-2}, \ldots,\tilde L_1$.


Writing
$$
\tilde L_j = I + \begin{bmatrix} ğŸ_j \\ \tilde \ell_{j+1,j} \\ â‹® \\ \tilde \ell_{n,j} \end{bmatrix} ğ_j^âŠ¤
$$
and using the properties of inversion and multiplication we therefore deduce
$$
L = \begin{bmatrix} 
1 \\ 
-\tilde \ell_{21} & 1 \\
-\tilde \ell_{31} & -\tilde \ell_{32} & 1 \\
 â‹® & â‹® & â‹± & â‹± \\
 -\tilde \ell_{n-1,1} & -\tilde \ell_{n-1,2} & â‹¯ &  - \tilde \ell_{n-1,n-2} & 1 \\
    -\tilde \ell_{n1} & -\tilde \ell_{n2} & â‹¯ &  - \tilde \ell_{n,n-2} &  -\ell_{n,n-1} & 1
\end{bmatrix}
$$



**Example 3**

Again we consider the matrix
$$
A = \begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix}
$$
Even though $a_{11} = 1 â‰  0$, we still pivot: placing 
the maximum entry on the diagonal to mitigate numerical errors.
That is, we first pivot and upper triangularise the first column:
$$
 L_1 P_1 A =  L_1\begin{bmatrix} 0 & 1 \\ 1 & 0 \\ && 1 \end{bmatrix}
\begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix} = 
                     \begin{bmatrix}1 \\ -{1 \over 2} & 1 \\ -{1 \over 2} && 1 \end{bmatrix}
\begin{bmatrix} 2 & 4 & 8 \\
                1 & 1 & 1 \\
                    1 & 4 & 9
                    \end{bmatrix}
$$
We now pivot and upper triangularise the second column:
$$
  L_2 P_2 L_1 P_1 A = 
                    L_2 \begin{bmatrix}
                    1 \\ &0 & 1 \\ &1 & 0 \end{bmatrix}
\begin{bmatrix} 2 & 4 & 8 \\
                0 & -1 & -3 \\
                    0 & 2 & 5
                    \end{bmatrix}
                    = \begin{bmatrix} 1\\ & 1 \\ & {1 \over 2} & 1 \end{bmatrix}
\begin{bmatrix} 2 & 4 & 8 \\
                0 & 2 & 5 \\
                0 & -1 & -3 
                    \end{bmatrix} = 
                    \underbrace{\begin{bmatrix} 2 & 4 & 8 \\
                0 & 2 & 5 \\
                0 & 0 & -{1 \over 2}
                    \end{bmatrix}}_U
$$
We now move $P_2$ to the right:
$$
L_2 P_2 L_1 P_1 = \underbrace{\begin{bmatrix} 1\\ -{1 \over 2} & 1 \\  -{1 \over 2}  & {1 \over 2} & 1 \end{bmatrix}}_{L_2 \tilde L_1} \underbrace{\begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}}_P
$$
That is
$$
L = \begin{bmatrix} 1\\ {1 \over 2} & 1 \\  {1 \over 2}  & -{1 \over 2} & 1 \end{bmatrix}
$$

We see how this example is done on a computer:
```julia
A = [1 1 1;
     2 4 8;
     1 4 9]
L,U,Ïƒ = lu(A) # Ïƒ is a vector encoding the permutation
```
The permutation is
```julia
Ïƒ
```
Thus to invert a system we can do:
```julia
b = randn(3)
U\(L\b[Ïƒ]) == A\b
```
Note the entries match exactly because this precisely what `\` is using.

## 3. Cholesky Factorisation

Cholesky Factorisation is a form of Gaussian elimination (without pivoting)
that exploits symmetry in the problem, resulting in a substantial speedup. 
It is only relevant for _symmetric positive definite_
matrices.

**Definition 1 (positive definite)** A square matrix $A âˆˆ â„^{n Ã— n}$ is _positive definite_ if
for all $ğ± âˆˆ â„^n, x â‰  0$ we have
$$
ğ±^âŠ¤ A ğ± > 0
$$

First we establish some basic properties of positive definite matrices:

**Proposition 3 (conj. pos. def.)** If  $A âˆˆ â„^{n Ã— n}$ is positive definite and 
$V âˆˆ â„^{n Ã— n}$ is non-singular then
$$
V^âŠ¤ A V
$$
is positive definite.

**Proposition 4 (diag positivity)** If $A âˆˆ â„^{n Ã— n}$ is positive definite
then its diagonal entries are positive: $a_{kk} > 0$.


**Theorem 1 (subslice pos. def.)** If $A âˆˆ â„^{n Ã— n}$ is positive definite
and $ğ¤ âˆˆ \{1,\ldots,n\}^m$ is a vector of $m$ integers where any integer appears only once,
 then $A[ğ¤,ğ¤] âˆˆ â„^{m Ã— m}$ is also
positive definite.



We leave the proofs to the problem sheets. Here is the key result:


**Theorem 2 (Cholesky and sym. pos. def.)** A matrix $A$ is symmetric positive definite if and only if it has a Cholesky factorisation
$$
A = L L^âŠ¤
$$
where the diagonals of $L$ are positive.

**Proof** If $A$ has a Cholesky factorisation it is symmetric ($A^âŠ¤ = (L L^âŠ¤)^âŠ¤ = A$) and for $ğ± â‰  0$ we have
$$
ğ±^âŠ¤ A ğ± = (Lğ±)^âŠ¤ L ğ± = \|Lğ±\|^2 > 0
$$
where we use the fact that $L$ is non-singular.

For the other direction we will prove it by induction, with the $1 Ã— 1$ case being trivial. 
Write
$$
A = \begin{bmatrix} Î± & ğ¯^âŠ¤ \\
                    ğ¯   & K
                    \end{bmatrix} = \underbrace{\begin{bmatrix} \sqrt{Î±} \\ 
                                    {ğ¯ \over \sqrt{Î±}} & I \end{bmatrix}}_{L_1}
                                    \underbrace{\begin{bmatrix} 1  \\ & K - {ğ¯ ğ¯^âŠ¤ \over Î±} \end{bmatrix}}_{A_1}
                                    \underbrace{\begin{bmatrix} \sqrt{Î±} & {ğ¯^âŠ¤ \over \sqrt{Î±}} \\
                                     & I \end{bmatrix}}_{L_1^âŠ¤}.
$$
Note that $K - {ğ¯ ğ¯^âŠ¤ \over Î±}$ is a subslice of $A_1 = L_1^{-1} A L_1^{-âŠ¤}$, hence by the previous propositions is
itself symmetric positive definite. Thus we can write 
$$
K - {ğ¯ ğ¯^âŠ¤ \over Î±} = \tilde L \tilde L^âŠ¤
$$
and hence $A = L L^âŠ¤$ for
$$
L= L_1 \begin{bmatrix}1 \\ & \tilde L \end{bmatrix}.
$$
satisfies $A = L L^âŠ¤$.
âˆ


N1te hidden in this proof is a simple algorithm form computing the Cholesky factorisation.
We define
$$
\begin{align*}
A_1 &:= A \\
ğ¯_k &:= A_k[2:n-k+1,1] \\
Î±_k &:= A_k[1,1] \\
A_{k+1} &:= A_k[2:n-k+1,2:n-k+1] - {ğ¯_k ğ¯_k^âŠ¤ \over Î±_k}.
\end{align*}
$$
Then
$$
L = \begin{bmatrix} \sqrt{Î±_1} \\
    {ğ¯_1[1] \over \sqrt{Î±_1}}  &  \sqrt{Î±_2} \\
    {ğ¯_1[2] \over \sqrt{Î±_1}}  & {ğ¯_2[1] \over \sqrt{Î±_2}} &  \sqrt{Î±_2} \\
    â‹® & â‹® & â‹± & â‹± \\
    {ğ¯_1[n-1] \over \sqrt{Î±_1}} &{ğ¯_2[n-2] \over \sqrt{Î±_2}} & â‹¯ & {ğ¯_{n-1}[1] \over \sqrt{Î±_{n-1}}} & \sqrt{Î±_{n}}
    \end{bmatrix}
$$

This algorithm succeeds if and only if $A$ is symmetric positive definite.

**Example 4** Consider the matrix
$$
A_0 = A = \begin{bmatrix}
2 &1 &1 &1 \\
1 & 2 & 1 & 1 \\
1 & 1 & 2 & 1 \\
1 & 1 & 1 & 2
\end{bmatrix}
$$
Then
$$
A_1 = \begin{bmatrix}
2 &1 &1 \\
1 & 2 & 1 \\
1 & 1 & 2 
\end{bmatrix} - {1 \over 2} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} =
{1 \over 2} \begin{bmatrix}
3 & 1 & 1 \\
1 & 3 & 1 \\
1 & 1 & 3 
\end{bmatrix}
$$
Continuing, we have 
$$
A_2 = {1 \over 2} \left( \begin{bmatrix}
3 & 1 \\ 1 & 3
\end{bmatrix} - {1 \over 3} \begin{bmatrix} 1 \\ 1  \end{bmatrix} \begin{bmatrix} 1 & 1  \end{bmatrix}
\right)
= {1 \over 3} \begin{bmatrix} 4 & 1 \\ 1 & 4 \end{bmatrix}
$$
Finally
$$
A_3 = {5 \over 4}.
$$
Thus we get
$$
L= L_1 L_2 L_3 = \begin{bmatrix} \sqrt{2} \\ {1 \over \sqrt{2}} & {\sqrt{3} \over 2} \\ 
{1 \over \sqrt{2}} & {1 \over \sqrt 6} & {2 \over \sqrt{3}} \\
{1 \over \sqrt{2}} & {1 \over \sqrt 6} & {1 \over \sqrt{12}} & {\sqrt{5} \over 2}
\end{bmatrix}
$$


# 3. Timings and Stability

The different factorisations have trade-offs between speed and stability.
First we compare the speed of the different factorisations on a symmetric positive
definite matrix, from fastest to slowest:

```julia
n = 100
A = Symmetric(rand(n,n)) + 100I # shift by 10 ensures positivity
@btime cholesky(A);
@btime lu(A);
@btime qr(A);
```
On my machine, `cholesky` is ~1.5x faster than `lu`,  
which is ~2x faster than QR. 



In terms of stability, QR computed with Householder reflections
(and Cholesky for positive definite matrices) are stable, 
whereas LU is usually unstable (unless the matrix
is diagonally dominant). PLU is a very complicated story: in theory it is unstable,
but the set of matrices for which it is unstable is extremely small, so small one does not
normally run into them.

Here is an example matrix that is in this set. 
```julia
function badmatrix(n)
    A = Matrix(1I, n, n)
    A[:,end] .= 1
    for j = 1:n-1
        A[j+1:end,j] .= -1
    end
    A
end
A =1badmatrix(5)
```
Note that pivoting will not occur (we do not pivot as the entries below the diagonal are the same magnitude as the diagonal), thus the PLU Factorisation is equivalent to an LU factorisation:
```julia
L,U = lu(A)
```
But here we see an issue: the last column of `U` is growing exponentially fast! Thus when `n` is large
we get very large errors:
```julia
n = 100
b = randn(n)
A = badmatrix(n)
norm(A\b - qr(A)\b) # A \ b still uses lu
```
Note `qr` is completely fine:
```julia
norm(qr(A)\b - qr(big.(A)) \b) # roughly machine precision
```

Amazingly, PLU is fine if applied to a small perturbation of `A`:
```julia
Îµ = 0.000001
AÎµ = A .+ Îµ .* randn.()
norm(AÎµ \ b - qr(AÎµ) \ b) # Now it matches!
```



The big _open problem_ in numerical linear algebra is to prove that the set of matrices
for which PLU fails has extremely small measure.


