## II.7 Condition numbers

We have seen that floating point arithmetic induces errors in computations, and that we can typically
bound the absolute errors to be proportional to $C Ïµ_{\rm m}$. We want a way to bound the
effect of more complicated calculations like computing $A ğ±$ or $A^{-1} ğ²$ without having to deal with
the exact nature of floating point arithmetic, as it will depend on the _data_ $A$ and $ğ±$. 
That is, we want to reduce floating point stability to a more fundamental property: _mathematical stability_:
how does a mathematical operation like $A ğ±$ change in the precense of  small perturbations (random noise or structured
floating point errors)?

1. Backward error analysis: We introduce the concept of _backward error_ analysis, which is a more practical
way of understanding and bounding floating point errors.
2. Condition numbers: We introduce a _condition numbers_, which can capture the effect of perturbations in
$A$ for linear algebra oerations. More precisely: matrix operations are mathematically _stable_ 
when the condition number is small.
3. Bounding floating point errors for linear algebra: we see how simple operations like $A ğ±$ can be put
into a backward error analysis framework, leading to bounds on the errors in terms of the condition number. 


## 1. Backward error analysis

So far we have done forward error analysis, e.g., to understand $f(x) â‰ˆ f^{\rm FP}(x)$ we consider either
the absolute
$$
f^{\rm FP}(x) = f(x) + Î´_{\rm a}
$$
or relative
$$
f^{\rm FP}(x) = f(x)(1 + Î´_{\rm r})
$$
errors of the _output_. More generally, for two vector spaces $V$ and $W$ (e.g. $V = â„^n$ and $W = â„^m$) consider functions $ğŸ = V â†’ W$. We write
$$
ğŸ^{\rm FP}(ğ±) = ğŸ(ğ±) + Î´_{\rm f}
$$
where we bound a norm of $Î´_{\rm f} âˆˆ W$ either _absolutely_:
$$
\|Î´_{\rm f}\|_W â‰¤ C Îµ
$$
or _relative_ to the true result:
$$
\|Î´_{\rm f}\|_W â‰¤ C \| ğŸ(ğ±) \|_W Îµ
$$
(which is similar to PS4, Q1.3).

On the other hand, _backward error analysis_ 
considers computations as errors in the _input_. That is, one writes
the approximate function as the true function with a pertubed input: e.g. find $ğ±Ìƒ âˆˆ V$ 
such that
$$
ğŸ^{\rm FP}(ğ±) = ğŸ(ğ±Ìƒ).
$$
We study the _backward  error_, the error in the input
$$
ğ±Ìƒ = ğ± + Î´_{\rm b}
$$
where $Î´_{\rm b} âˆˆ â„^n$ by bounding either the absolute error
$$
\|Î´_{\rm b}\|_V â‰¤ CÎµ
$$
or relative error:
$$
Î´_{\rm b}\|_V â‰¤ C \|ğ±\|_V Îµ
$$
We shall see that _some_ algorithms (like `mul_rows`) lead naturally
to backward error results. 



## 2. Condition numbers

So now we get to a mathematical question independent of floating point: 
can we bound the _relative error_ in approximating
$$
A ğ± â‰ˆ (A + Î´A) ğ±
$$
if we know a bound on the relative backward error   $\|Î´A\|$?
It turns out we can in turns of the _condition number_ of the matrix:

**Definition 2 (condition number)**
For a square matrix $A$, the _condition number_ (in $p$-norm) is
$$
Îº_p(A) := \| A \|_p \| A^{-1} \|_p
$$
with the default being the $2$-norm condition number, writable in terms of the
singular values as:
$$
Îº(A) := Îº_2(A) = \| A \|_2 \| A^{-1} \|_2 = {Ïƒ_1 \over Ïƒ_n}.
$$


**Theorem 1 (relative forward error for matrix-vector)**
Assume we have the relative backward error bound $\|Î´A\| â‰¤ \|A \| Îµ$.
Then for 
$$
(A + Î´A) ğ± = A ğ± + Î´_{\rm f}
$$
the forward error has the relative error bound
$$
\|Î´_{\rm f}\|  â‰¤ \| A ğ± \| Îº(A) Îµ
$$


**Proof**
We can assume $A$ is invertible (as otherwise $Îº(A) = âˆ$). Denote $ğ² = A ğ±$ and we have
$$
{\|ğ± \| \over \| A ğ± \|} = {\|A^{-1} ğ² \| \over \|ğ² \|} â‰¤ \| A^{-1}\|
$$
Thus we have:
$$
{\|Î´_{\rm f}\|  \over \| A ğ± \|} â‰¤ {\| Î´A \| \| ğ± \| \over \| A ğ± \| } â‰¤ \underbrace{\|A \| \|A^{-1}\|}_{Îº(A)} Îµ.
$$

âˆ



## 3. Bounding floating point errors for linear algebra

We now observe that errors in implementing matrix-vector multiplication using floating points
can be captured by considering the multiplication to be exact on the wrong matrix: that is, `A*x`
(implemented with floating point as `mul_rows`) is precisely $A + Î´A$ where $Î´A$ has small norm, relative to $A$.
That is, we have a bound on the _backward relative error_.



To discuss floating point errors we need to be precise which order the operations happened.
We will use the definition `mul_rows(A,x)` (which is equivalent to `mul_cols(A,x)`).
 Note that each entry of the result is in fact a dot-product
of the corresponding rows so we first consider the error in the dot product  `dot(ğ±,ğ²)` as implemented in floating-point:
$$
{\rm dot}(ğ±,ğ²) = â¨_{k=1}^n (x_k âŠ— y_k).
$$

We first need a helper proposition:

**Proposition 1 [PS2 Q2.1]** If $|Ïµ_i| â‰¤ Ïµ$ and $n Ïµ < 1$, then
$$
\prod_{k=1}^n (1+Ïµ_i) = 1+Î¸_n
$$
for some constant $Î¸_n$ satisfying
$$
|Î¸_n| â‰¤ \underbrace{n Ïµ \over 1-nÏµ}_{E_{n,Ïµ}}.
$$



**Lemma 1 (dot product backward error)**
For $ğ±, ğ² âˆˆ â„^n$,
$$
{\rm dot}(ğ±, ğ²) = (ğ± + Î´ğ±)^âŠ¤ ğ²
$$
where, assuming $n Ïµ_{\rm m} < 2$, the entries satisfy
$$
|Î´x_k| â‰¤  E_{n,Ïµ_{\rm m}/2} |x_k |.
$$


**Proof**

This is related to PS2 Q2.3 but asks for the _backward error_ instead of the
_forward error_. Note
$$
{\rm dot}(ğ±, ğ²) = â¨_{j=1}^n (x_j âŠ— y_j) = â¨_{j=1}^n (x_j  y_j) (1 + Î´_j)
= x_1 y_1 (1 + Î¸Ìƒ_{n}) +   âˆ‘_{j=2}^n x_j y_j (1 + Î¸_{n-j+2})
$$
where $|Î¸Ìƒ_n|, |Î¸_k| â‰¤ E_{n,Ïµ_{\rm m}/2}$ (the subscript denotes the number of terms
bounded by $Îµ_{\rm m}/2$). Thus we can define
$$
Î´ğ±  := \begin{bmatrix}
x_1 Î¸Ìƒ_n \\
x_2 Î¸_n \\
â‹® \\
x_n Î¸_2
\end{bmatrix}
$$
where
$$
| Î´x_k | â‰¤  E_{n,Ïµ_{\rm m}/2} | x_k |.
$$


âˆ

We can use this to get a relative backward error bound on `mul_rows`:

**Theorem 2 (matrix-vector backward error)**
For $A âˆˆ â„^{m Ã— n}$ and $ğ± âˆˆ â„^n$ (both with normal float entries) we have
$$
\hbox{mul$\_$rows}(A, ğ±) = (A + Î´A) ğ±
$$
where, assuming $n Ïµ_{\rm m} < 2$ and all operations are in the normalised range,
the entries (denoting $Î´a_{kj} = Î´A[k,j] = ğ_k^âŠ¤ Î´A ğ_j$) satisfy
$$
|Î´a_{kj}| â‰¤ E_{n,Ïµ_{\rm m}/2}  |a_{kj}|.
$$

**Proof**
The bound on the entries of $Î´A$ is implied by the previous lemma
since each row is equivalent to a dot product.
âˆ

**Corollary 1 (Norms)**
$$
\begin{align*}
\|Î´A\|_1 &â‰¤ E_{n,Ïµ_{\rm m}/2} \|A \|_1 \\
\|Î´A\|_2 &â‰¤ \sqrt{\min(m,n)} E_{n,Ïµ_{\rm m}/2} \|A \|_2 \\
\|Î´A\|_âˆ &â‰¤ E_{n,Ïµ_{\rm m}/2} \|A \|_âˆ
\end{align*}
$$
In particular, 
$$
\hbox{mul$\_$rows}(A, ğ±) =A ğ± + Î´_{\rm f}
$$
where
$$
\|Î´_{\rm f}\| â‰¤ \|A ğ±\| Îº(A) E_{n,Ïµ_{\rm m}/2}
$$

**Proof**


The $1$-norm follow since
$$
\|Î´A\|_1  = \max_j âˆ‘_{k=1}^m |Î´a_{kj}| â‰¤
 E_{n,Ïµ_{\rm m}/2}  \max_j âˆ‘_{k=1}^m |a_{kj}|  = E_{n,Ïµ_{\rm m}/2} \|A\|_1
$$
and the proof for the $âˆ$-norm is similar.


This leaves the 2-norm, which is a bit more challenging.
We will prove the result by going through the FrÃ¶benius norm and using
$$
\|A \|_2 â‰¤ \|A\|_F â‰¤ \sqrt{r} \| A\|_2
$$
where $r$ is rank of $A$ (see PS6 Q5.2).
So we deduce
$$
\begin{align*}
\|Î´A \|_2^2 &â‰¤ \| Î´A\|_F^2 = âˆ‘_{k=1}^m âˆ‘_{j=1}^n |Î´a_{kj}|^2 â‰¤
E_{n,Ïµ_{\rm m}/2}^2 âˆ‘_{k=1}^m âˆ‘_{j=1}^n |a_{kj}|^2 \\
&=  E_{n,Ïµ_{\rm m}/2}^2 \|A \|_F^2 â‰¤ E_{n,Ïµ_{\rm m}/2}^2 r \|A \|_2^2.
\end{align*}
$$
and the rank of $A$ is bounded by $\min(m,n)$.
The bound on the forward error then follows from Theorem 1.

âˆ


We can also bound the error of back-substitution in terms of the condition number (see PS7).
If one uses QR to solve $A ğ± = ğ²$ the condition number also gives a meaningful bound on the error. 
As we have already noted, there are some matrices where PLU decompositions introduce large errors, so
in that case well-conditioning is not a guarantee  of accuracy (but it still usually works).
